
\documentclass[twocolumn]{article}

\usepackage{dcj}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{graphicx}

\title{Compression of next-generation sequencing reads aided by highly efficient de novo assembly}
\author{Daniel C. Jones}

\begin{document}

\maketitle

\section{Abstract}

We present Quip, an extremely efficient compression algorithm for
high-throughput sequencing data.  We use reference-based compression for aligned
reads in the SAM/BAM format to avoid storing redundant sequences.  Unaligned
reads are assembled into contigs using an novel de novo assembly algorithm that
far more space efficient than traditional assemblers.  Read identifiers and
quality scores are compressed using arithmetic coding with straightforward
statistical modeling.

Availability: Quip is freely available under an open-source license from
\url{http://cs.washington.edu/homes/dcjones/quip}.


\section{Introduction}

%% Notes on tho cost of sequencing versus the cost of disk storage.

%% Historical work compressing DNA


%% Formats

For its simplicity, FASTQ is a surprisingly ill-defined format. The closest
thing to an accepted specification is the description by \citet{Cock2010}. The
format arose ad hoc from multiple sources (primarily Sanger and
Solexa/Illumina), so this specification in descriptive rather than
prescriptive.

The SAM format is both more complex and more tightly defined, and comes with a
reference implementation in the form of SAMtools \citep{Li2009b}. It is able to
store alignment information in addition to read identifiers, sequences, and
quality scores. In addition, SAM files can be converted to the BAM formate, a
compressed binary version of SAM, which is far more compact and allows for
relatively efficient random access. 


%% Referenced based compression

This idea is explored also in the Goby format \citep{Goby2012}, in which,
unlike the SAM/BAM format, the sequences of aligned reads are not stored but
looked up in a reference genome when needed. For some applications reference-
based compression can be taken much further by storing only SNP information,
summarizing a sequencing experiment in mere kilobytes \citep{Christley2009}.
However, even in these cases, discarding the raw reads would prevent any
reanalysis of the data. 

%% Recent work compressing short reads

A common theme in the growing literature on on short read compression is lossy
encoding of sequence quality scores. \citet{Wan2011} explored three binning
schemes that reduce the alphabet size of the quality scores before encoding
them with one of several general purpose compression methods.

Something about \citet{Hach2012}.

%% XXX: who else?




%% Summarize literature of lossy quality compression here.

This follows naturally from the realization that quality scores are
particularly difficult to compress. Unlike read identifiers, which are highly
redundant, or nucleotide sequences which XXX, quality scores are
inconsistently encoded between protocols and computational pipelines and are
often simply high-entropy. It is dissatisfying that metadata (quality scores)
should consume more space than primary data (nucleotide sequences). Yet, also
dissatisfying to many researchers is the thought of discarding information
without a very good understanding of its affect on downstream analysis.
Decreasing the entropy of quality scores while retaining accuracy is an
worthwhile endeavor, but difficult given that both the algorithms that
generate them and the algorithms that are informed by them are moving targets.


\section{Methods}

\subsection{Reference Based Compression}

\subsection{Statistical Compression with Arithmetic Coding}

\subsubsection{Read Identifiers}

\subsubsection{Nucleotide Sequences}

\subsubsection{Quality Scores}


\subsection{Probabalistic De Bruijn Graph Assembly}

% Though probabilistic data structures are 


\section{Results}

%% Chosen datasets.

\subsection{Compression of Unaligned Reads}

\begin{figure*}
\centerline{\includegraphics[width=\textwidth]{analysis/sizes.pdf}}
\caption{TODO}
\label{fig:sizes}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=\textwidth]{analysis/comp_time.pdf}}
\centerline{\includegraphics[width=\textwidth]{analysis/decomp_time.pdf}}
\caption{TODO}
\label{fig:comp_decomp_time}
\end{figure*}

\subsection{Reference-based Compression of Aligned Reads}

\subsection{Characteristics of the d-left Counting Bloom Filter}

Though our primary goal is efficient compression of sequencing data, the
assembly algorithm we developed to achieve this is of independent interest.
Only very recently has the idea of using probabilistic data structures in
assembly been breached, and to our knowledge, we are the first to build
a functioning assembler using the dlCBF.

The precise false positive rate of the data structure is unimportant when the
goal is compression, but if the method is to be extended to perform actual
analysis, it becomes a serious concern. The false positive rate of a Bloom
filter can be obtained analytically, though doing so is not entirely trivial.

Comparisons of data structure performance are notoriously sensitive to the
particulars of the implementation. To perform a meaningful benchmark, we
compared our dlCBF implementation to the sparsehash library, an open source
hash table implementation with the expressed goal of maximizing space
efficiency. Among many other uses, it is the core data structure in the ABySS
\citep{Simpson2011} and PASHA \citep{Liu2011} assemblers.

We randomly generated 10 million unique 25-mers and inserted them into a hash
table sized appropriately as to avoid expansion and rehashing. We repeated
this with dlCBF tables of increasing sizes. Upon insertion, a false positive
occurs when the hash functions computed on the inserted $k$-mer collide with a
previously inserted $k$-mer. An insertion may also fail when a fixed size
table is filled to capacity and no empty cells are available. For simplicity,
we count these occurrences as ``false positives''. Run time and maximum memory
usage were both recorded using the Unix time command.

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{analysis/dlcbf/benchmark.pdf}}
\caption{
The trade-off between memory usage and false positive rate in the dlCBF is
evaluated by inserting 10 million unique 25-mers into tables of increasing
size. Memory usage in reported as the proportion of the memory used by a
memory efficient hash table to do the same.
}
\label{fig:dlcbf_bench}
\end{figure}

We find that with only 20\% of the space of the hash table the dlCBF accrues a
false positive rate of less than 0.001 (Figure \ref{fig:dlcbf_bench}). While
the hash table performed the 10 million insertions in 7.34 seconds, it
required only 0.61 seconds on average for the dlCBF to do the same on a 2.8Ghz
Intel Xeon processor. Table size did not greatly affect the runtime of the
dlCBF.

Though the authors of sparsehash claim only a 4 bit overhead for each entry,
and have gone to considerably effort to achieve such efficiency, it still must
store the $k$-mer itself, encoded in 64 bits. The dlCBF avoids this, storing
instead a 14-bit ``fingerprint'', or hash of a $k$-mer, resulting in the large
savings we observe. Of course, a 25-mer cannot be uniquely identified with 14
bits. False positives are thus introduced, yet they are kept at a very low
rate by the d-left hashing scheme. Since multiple hash functions are used
under this scheme, multiple collisions must occur to result in a false
positive; an infrequent event if reasonably high-quality hash functions are
chosen.

A previous analysis of the dlCBF by \citet{Zhang2009} compared it to two other
variations of the counting Bloom filter and concluded that ``the dlCBF
outperforms the others remarkably, in terms of both space efficiency and
accuracy.'' Overall, this data structure appears particularly adept for high
efficiency de novo assembly.

\section{Discussion}

%% Points to make
% We are doing de novo assembly faster than gzip can compress!!
% DSRC support random access, but there is never a need for this with FASTQ.

The Lempel-Ziv algorithm, particularly as implemented in gzip/libz has become
a standard choice for compression. The zlib library has matured and stabilized
over the course of two decades and is widely available. The BAM and Goby
formats both use zlib for compression, and compressing FASTQ files with gzip
is still common practice. Despite its ubiquity, our benchmarks show that it is
remarkably poorly suited to NGS data. Both compression ratio and compression
time were inferior to the other programs evaluated. For most purposes, the
gains in decompression time do not make up for its shortcomings.

Using a more sophisticated variation, the Lempel-Ziv-Markov algorithm
implemented in xz results in a significant increase in compression, but at the
cost of tremendously slow compression (often an entire day is required to
compress a single lane from a HiSeq 2000). This is not a great surprise. The
Lempel-Ziv algorithm works by matching repeated substrings in the input
stream; an extremely effective technique for compressing text. When the data
is not so highly structured, this technique breaks down.

%% Something else here?

Our use of high-order Markov chain models and de novo assembly results in a
program that uses significantly more memory than the others tested. Though
limiting the memory used by a general purpose compression program enables it
to be used on a wider variety of systems, this is less important in this
domain-specific application. Common analysis of next-generation sequencing
data, whether it be alignment, assembly, isoform quantification, peak calling,
or SNP calling all require significant computational resources. Targeting
low-memory systems would not be of particular benefit: next-generation
sequencing precludes previous-generation hardware.


\bibliographystyle{plainnat}
\bibliography{quip}

\end{document}



