
\documentclass[twocolumn]{article}

\usepackage{dcj}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{graphicx}

\title{Compression of next-generation sequencing reads aided by highly efficient de novo assembly}
\author{Daniel C. Jones}

\begin{document}

\maketitle

\section{Abstract}

We present Quip, an extremely efficient compression algorithm for
high-throughput sequencing data.  We use reference-based compression for aligned
reads in the SAM/BAM format to avoid storing redundant sequences.  Unaligned
reads are assembled into contigs using an novel de novo assembly algorithm that
far more space efficient than traditional assemblers.  Read identifiers and
quality scores are compressed using arithmetic coding with straightforward
statistical modeling.

Availability: Quip is freely available under an open-source license from
\url{http://cs.washington.edu/homes/dcjones/quip}.


\section{Introduction}

% Some blathering about NGS

% Disk storage costs.

% Formats

For its simplicity, FASTQ is a surprisingly ill-defined format. The closest
thing to an accepted specification is the description by \citet{Cock2010}. The
format arose ad hoc from multiple sources (primarily Sanger and
Solexa/Illumina), so this specification in descriptive rather than
prescriptive.

The SAM format is both more complex and more tightly defined, and comes with a
reference implementation in the form of SAMtools \citep{Li2009b}. It is able to
store alignment information in addition to read identifiers, sequences, and
quality scores. In addition, SAM files can be converted to the BAM format, a
compressed binary version of SAM, which is far more compact and allows for
relatively efficient random access. 

% DNA Compression

% NGS Compression

% Referenced-based compression

The fundamental characteristic of NGS data is that it consists of millions of
fragments of greater whole, whether it be a genome sequence or spliced mRNA
transcripts. This realization led \citet{Kozanitis2011} and
\citet{Hsi-YangFritz2011} to 



%% TODO: Fritz, et al

This idea is explored also in the Goby format \citep{Goby2012}, which has been
proposed an an alternative to SAM/BAM, the primary functional difference being
that sequences of aligned reads are not stored but looked up in a reference
genome when needed. For some applications reference-based compression can be
taken much further by storing only SNP information, summarizing a sequencing
experiment in mere kilobytes \citep{Christley2009}. However, even when SNP
calls are all that is needed, discarding the raw reads would prevent any
reanalysis of the data.


% Lossy compression of quality scores

A reoccurring theme in the  the growing literature on on short read
compression is lossy encoding of sequence quality scores. This follows
naturally from the realization that quality scores are particularly difficult
to compress. Unlike read identifiers, which are highly redundant, or
nucleotide sequences which XXX, quality scores are inconsistently encoded
between protocols and computational pipelines and are often simply high-
entropy. It is dissatisfying that metadata (quality scores) should consume
more space than primary data (nucleotide sequences). Yet, also dissatisfying
to many researchers is the thought of discarding information without a very
good understanding of its affect on downstream analysis.

A number of lossy compression algorithms for quality scores have been
proposed, including various binning schemes to reduce the alphabet size
\citep{Wan2011}, scaling to a reduced alphabet with randomized rounding
\citep{Kozanitis2011}, and discarding quality scores for bases which match a
reference sequence \citep{Hsi-YangFritz2011}. Only \citet{Kozanitis2011} go to
the effort of evaluating the effects of their algorithm on downstream
analysis. Their results suggest that while some SNP calls are affected, they
are primarily marginal, low-confidence calls between hetero- and homozygosity.

% TODO: Something about \citet{Hach2012}. What is their method doing?

Decreasing the entropy of quality scores while retaining accuracy is an
important goal, but successful lossy compression demands an understanding of
what is lost. For example, lossy audio compression (e.g. MP3) is grounded in
psychoacoustic principles, preferentially discarding the least perceptible
sound. Conjuring a similarly principled method for NGS quality scores is
difficult given that both the algorithms that generate them and the algorithms
that are informed by them are moving targets. In the analysis by
\citet{Kozanitis2011}, the authors are appropriately cautious in interpreting
their results, pointing our that ``there are dozens of downstream applications
and much work needs to be done to ensure that coarsely quantized quality
values will be acceptable for users.''


% Quip to the rescue!


Though the method we propose here is entirely lossless, it is suitably general
(as is any method based on entropy encoding) so that any lossy transformation
applied upstream of compression can be automatically exploited.


\section{Methods}

\subsection{Statistical Compression with Arithmetic Coding}

Entropy encoding is a particularly elegant means of compression in that it
allows a distinct separation between modeling and encoding. In Quip, the same
arithmetic coder is used to encode quality scores, read identifiers,
nucleotide sequences, and alignment information, but with very different
statistical models for each.

\subsubsection{Read Identifiers}

\subsubsection{Nucleotide Sequences}

\subsubsection{Quality Scores}


\subsection{Reference Based Compression}

\subsection{Assembly Based Compression}

% This can be thought of as generalization of the Lempel-Ziv algorithm, whereas,
% rather than attempting to match strings to previous occurrences, we match strings
% to contigs  ...


\subsubsection{Probabalistic De Bruijn Graph Assembly}


Counting $k$-mers efficiently with the help of Bloom filters was explored by
\citet{Melsted2011}.

Concurrently with our work, \citet{Pell2011} have also developed a
probabilistic De Bruijn graph assembler using a (non-counting) Bloom filter.
While they demonstrate a significant reduction in memory use, unlike other De
Bruijn graph assemblers, only the presence or absence of a k-mer is stored, not
its abundance, which is essential information when the goal is producing
accurate contigs. Nevertheless, their analysis suggests errors introduced by
false-positives in the Bloom filter are low when compared to sequencing
errors. This does not directly apply to the data structure used here, but in
Section XXX we demonstrate that the false-positive rate the dlCBF can be kept
exceedingly low, while using a little as an 20\% of the memory of a
particularly space-efficient hash table.

% Though probabilistic data structures are 


\section{Results}

%% Chosen datasets.

\subsection{Compression of Unaligned Reads}

\begin{figure*}
\centerline{\includegraphics[width=\textwidth]{analysis/sizes.pdf}}
\caption{TODO}
\label{fig:sizes}
\end{figure*}

\begin{figure*}
\centerline{\includegraphics[width=\textwidth]{analysis/comp_time.pdf}}
\centerline{\includegraphics[width=\textwidth]{analysis/decomp_time.pdf}}
\caption{TODO}
\label{fig:comp_decomp_time}
\end{figure*}

\subsection{Reference-based Compression of Aligned Reads}

Reference-based compression was first proposed by \citet{Hsi-YangFritz2011}
and later implemented in the cramtools package. This technique has also been explored
by 

\subsection{Characteristics of the d-left Counting Bloom Filter}

Though our primary goal is efficient compression of sequencing data, the
assembly algorithm we developed to achieve this is of independent interest.
Only very recently has the idea of using probabilistic data structures in
assembly been breached, and to our knowledge, we are the first to build
a functioning assembler using the dlCBF.

The precise false positive rate of the data structure is unimportant when the
goal is compression, but if the method is to be extended to perform actual
analysis, it becomes a serious concern. The false positive rate of a Bloom
filter can be obtained analytically, though doing so is not entirely trivial.

Comparisons of data structure performance are notoriously sensitive to the
particulars of the implementation. To perform a meaningful benchmark, we
compared our dlCBF implementation to the sparsehash library, an open source
hash table implementation with the expressed goal of maximizing space
efficiency. Among many other uses, it is the core data structure in the ABySS
\citep{Simpson2011} and PASHA \citep{Liu2011} assemblers.

We randomly generated 10 million unique 25-mers and inserted them into a hash
table sized appropriately as to avoid expansion and rehashing. We repeated
this with dlCBF tables of increasing sizes. Upon insertion, a false positive
occurs when the hash functions computed on the inserted $k$-mer collide with a
previously inserted $k$-mer. An insertion may also fail when a fixed size
table is filled to capacity and no empty cells are available. For simplicity,
we count these occurrences also as ``false positives''. Run time and maximum memory
usage were both recorded using the Unix time command.

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{analysis/dlcbf/benchmark.pdf}}
\caption{
The trade-off between memory usage and false positive rate in the dlCBF is
evaluated by inserting 10 million unique 25-mers into tables of increasing
size. Memory usage in reported as the proportion of the memory used by a
memory efficient hash table to do the same.
}
\label{fig:dlcbf_bench}
\end{figure}

We find that with only 20\% of the space of the hash table the dlCBF accrues a
false positive rate of less than 0.001 (Figure \ref{fig:dlcbf_bench}). While
the hash table performed the 10 million insertions in 7.34 seconds, it
required only 0.61 seconds on average for the dlCBF to do the same on a 2.8Ghz
Intel Xeon processor. Table size did not greatly affect the runtime of the
dlCBF.

Though the authors of sparsehash claim only a 4 bit overhead for each entry,
and have gone to considerably effort to achieve such efficiency, it still must
store the $k$-mer itself, encoded in 64 bits. The dlCBF avoids this, storing
instead a 14-bit ``fingerprint'', or hash of a $k$-mer, resulting in the large
savings we observe. Of course, a 25-mer cannot be uniquely identified with 14
bits. False positives are thus introduced, yet they are kept at a very low
rate by the d-left hashing scheme. Since multiple hash functions are used
under this scheme, multiple collisions must occur to result in a false
positive; an infrequent event if reasonably high-quality hash functions are
chosen.

A previous analysis of the dlCBF by \citet{Zhang2009} compared it to two other
variations of the counting Bloom filter and concluded that ``the dlCBF
outperforms the others remarkably, in terms of both space efficiency and
accuracy.'' Overall, this data structure appears particularly adept for high
efficiency de novo assembly.

\section{Discussion}

%% Points to make
% We are doing de novo assembly faster than gzip can compress!!
% DSRC support random access, but there is never a need for this with FASTQ.

% Lempel-Ziv is ill-suited to NGS data

The Lempel-Ziv algorithm, particularly as implemented in gzip/libz has become
a reasonable default choice for compression. The zlib library has matured and stabilized
over the course of two decades and is widely available. The BAM and Goby
formats both use zlib for compression, and compressing FASTQ files with gzip
is still common practice. Despite its ubiquity, our benchmarks show that it is
remarkably poorly suited to NGS data. Both compression ratio and compression
time were inferior to the other programs evaluated. For most purposes, the
gains in decompression time do not make up for its shortcomings.

Using a more sophisticated variation, the Lempel-Ziv-Markov algorithm
implemented in xz results in a significant increase in compression, but at the
cost of tremendously slow compression (often an entire day is required to
compress a single lane from a HiSeq 2000). This is not a great surprise: the
Lempel-Ziv algorithm works by matching repeated substrings in the input
stream; This is an extremely effective technique for compressing text, but
when the data is not so highly structured, it breaks down.

%% Something else here?


% Memory/Time Usage

Our use of high-order Markov chain models and de novo assembly results in a
program that uses significantly more memory than the others tested, with the
exception of cramtools. Though limiting the memory used by a general purpose
compression program enables it to be used on a wider variety of systems, this
is less important in this domain-specific application. Common analysis of
next-generation sequencing data, whether it be alignment, assembly, isoform
quantification, peak calling, or SNP calling all require significant
computational resources. Targeting low-memory systems would not be of
particular benefit: next-generation sequencing precludes previous-generation
hardware.

Though memory consumption is not a top priority, runtime is important. Newer
instruments like the HiSeq 2000 produce far more data per lane than previously
possible. And, as the cost of sequencing continues to decrease, experiments
will involve more conditions, replicates, and timepoints.  Quip is able to
compress NGS data at three times the speed of gzip, while performing de novo
assembly of millions of reads, and up to five times as fast without the
assembly step. Only DSRC is faster, but with consistently lower compression.
In addition, our reference-based compression algorithm is nearly twice as fast
as cramtools, with substantially better lossless compression.


% Conclusion

\bibliographystyle{plainnat}
\bibliography{quip}

\end{document}



