@article{Armbrust2009,
abstract = {Marine diatoms rose to prominence about 100 million years ago and today generate most of the organic matter that serves as food for life in the sea. They exist in a dilute world where compounds essential for growth are recycled and shared, and they greatly influence global climate, atmospheric carbon dioxide concentration and marine ecosystem function. How these essential organisms will respond to the rapidly changing conditions in today's oceans is critical for the health of the environment and is being uncovered by studies of their genomes.},
author = {Armbrust, E Virginia},
doi = {10.1038/nature08057},
file = {::},
issn = {1476-4687},
journal = {Nature},
keywords = {Animals,Biological Evolution,Carbon Dioxide,Carbon Dioxide: metabolism,Diatoms,Diatoms: chemistry,Diatoms: classification,Diatoms: genetics,Diatoms: metabolism,Ecosystem,Food Chain,Glass,Humans,Iron,Iron: metabolism,Marine Biology},
month = may,
number = {7244},
pages = {185--92},
pmid = {19444204},
title = {{The life of diatoms in the world's oceans.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19444204},
volume = {459},
year = {2009}
}
@inproceedings{Pfau2010,
author = {Bartlett, Nicholas and Pfau, David and Wood, Frank},
booktitle = {Proceeding of the 27th International Conference on Machine Learning (ICML)},
file = {::},
title = {{Forgetting Counts : Constant Memory Inference for a Dependent Hierarchical Pitman-Yor Process}},
year = {2010}
}
@article{Bhola2011,
author = {Bhola, Vishal and Bopardikar, Ajit S. and Narayanan, Rangavittal and Lee, Kyusang and Ahn, TaeJin},
doi = {10.1109/BIBM.2011.110},
file = {::},
isbn = {978-1-4577-1799-4},
journal = {2011 IEEE International Conference on Bioinformatics and Biomedicine},
keywords = {-fastq,genomic,next generation sequencing},
month = nov,
pages = {147--150},
publisher = {Ieee},
title = {{No-Reference Compression of Genomic Data Stored in FASTQ Format}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6120426},
year = {2011}
}
@article{Brandon2009,
abstract = {The continuing exponential accumulation of full genome data, including full diploid human genomes, creates new challenges not only for understanding genomic structure, function and evolution, but also for the storage, navigation and privacy of genomic data. Here, we develop data structures and algorithms for the efficient storage of genomic and other sequence data that may also facilitate querying and protecting the data.},
author = {Brandon, Marty C and Wallace, Douglas C and Baldi, Pierre},
doi = {10.1093/bioinformatics/btp319},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics},
keywords = {Algorithms,DNA,DNA: methods,Data Compression,Data Compression: methods,Genome,Genomics,Genomics: methods,Humans,Sequence Analysis},
month = jul,
number = {14},
pages = {1731--8},
pmid = {19447783},
title = {{Data structures and compression algorithms for genomic sequence data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2705231\&tool=pmcentrez\&rendertype=abstract},
volume = {25},
year = {2009}
}
@article{Butler2008,
abstract = {New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun "microreads." For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80x coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads.},
author = {Butler, Jonathan and MacCallum, Iain and Kleber, Michael and Shlyakhter, Ilya a and Belmonte, Matthew K and Lander, Eric S and Nusbaum, Chad and Jaffe, David B},
doi = {10.1101/gr.7337908},
file = {::},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Campylobacter jejuni,Campylobacter jejuni: genetics,Computational Biology,Computational Biology: methods,Computer Simulation,Escherichia coli,Escherichia coli: genetics,Genome, Bacterial,Genome, Bacterial: genetics,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Sequence Analysis, DNA: standards},
month = may,
number = {5},
pages = {810--20},
pmid = {18340039},
title = {{ALLPATHS: de novo assembly of whole-genome shotgun microreads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2336810\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Chaisson2008,
abstract = {In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.},
author = {Chaisson, Mark J and Pevzner, Pavel a},
doi = {10.1101/gr.7088808},
file = {::},
issn = {1088-9051},
journal = {Genome Research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Contig Mapping,Contig Mapping: methods,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Research Design,Sequence Analysis},
month = feb,
number = {2},
pages = {324--30},
pmid = {18083777},
title = {{Short read fragment assembly of bacterial genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2203630\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@inproceedings{Chen2004,
abstract = {We propose derivative Boyer-Moore (d-BM), a new compressed pattern matching algorithm in DNA sequences. This algorithm is based on the Boyer-Moore method, which is one of the most popular string matching algorithms. In this approach, we compress both DNA sequences and patterns by using two bits to represent each A, T, C, G character. Experiments indicate that this compressed pattern matching algorithm searches long DNA patterns (length > 50) more than 10 times faster than the exact match routine of the software package Agrep, which is known as the fastest pattern matching tool. Moreover, compression of DNA sequences by this method gives a guaranteed space saving of 75\%. In part the enhanced speed of the algorithm is due to the increased efficiency of the Boyer-Moore method resulting from an increase in alphabet size from 4 to 256.},
author = {Chen, Lei and Lu, Shiyong and Ram, Jeffrey},
booktitle = {Proceedings of the IEEE Computational Systems Bioinformatics Conference (CSB)},
file = {::},
isbn = {0769521940},
issn = {1551-7497},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Base Sequence,Chromosome Mapping,Chromosome Mapping: methods,DNA,DNA: methods,Data Compression,Data Compression: methods,Molecular Sequence Data,Pattern Recognition,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis},
month = jan,
number = {Csb},
pages = {62--8},
pmid = {16448000},
title = {{Compressed pattern matching in DNA sequences.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16448000},
year = {2004}
}
@article{Compeau2011,
author = {Compeau, Phillip E C and Pevzner, Pavel a and Tesler, Glenn},
doi = {10.1038/nbt.2023},
file = {::},
isbn = {0000110010111},
issn = {1087-0156},
journal = {Nature Biotechnology},
month = nov,
number = {11},
pages = {987--991},
publisher = {Nature Publishing Group},
title = {{How to apply de Bruijn graphs to genome assembly}},
url = {http://www.nature.com/doifinder/10.1038/nbt.2023},
volume = {29},
year = {2011}
}
@article{Croft2005,
abstract = {Vitamin B12 (cobalamin) was identified nearly 80 years ago as the anti-pernicious anaemia factor in liver, and its importance in human health and disease has resulted in much work on its uptake, cellular transport and utilization. Plants do not contain cobalamin because they have no cobalamin-dependent enzymes. Deficiencies are therefore common in strict vegetarians, and in the elderly, who are susceptible to an autoimmune disorder that prevents its efficient uptake. In contrast, many algae are rich in vitamin B12, with some species, such as Porphyra yezoensis (Nori), containing as much cobalamin as liver. Despite this, the role of the cofactor in algal metabolism remains unknown, as does the source of the vitamin for these organisms. A survey of 326 algal species revealed that 171 species require exogenous vitamin B12 for growth, implying that more than half of the algal kingdom are cobalamin auxotrophs. Here we show that the role of vitamin B12 in algal metabolism is primarily as a cofactor for vitamin B12-dependent methionine synthase, and that cobalamin auxotrophy has arisen numerous times throughout evolution, probably owing to the loss of the vitamin B12-independent form of the enzyme. The source of cobalamin seems to be bacteria, indicating an important and unsuspected symbiosis.},
author = {Croft, Martin T and Lawrence, Andrew D and Raux-Deery, Evelyne and Warren, Martin J and Smith, Alison G},
doi = {10.1038/nature04056},
file = {::},
issn = {1476-4687},
journal = {Nature},
keywords = {5-Methyltetrahydrofolate-Homocysteine S-Methyltran,Bacteria,Bacteria: cytology,Bacteria: growth \& development,Bacteria: metabolism,Coculture Techniques,Eukaryota,Eukaryota: classification,Eukaryota: cytology,Eukaryota: genetics,Eukaryota: metabolism,Genome,Genomics,Halomonas,Halomonas: cytology,Halomonas: growth \& development,Halomonas: metabolism,Symbiosis,Vitamin B 12,Vitamin B 12: metabolism},
month = nov,
number = {7064},
pages = {90--3},
pmid = {16267554},
title = {{Algae acquire vitamin B12 through a symbiotic relationship with bacteria.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16267554},
volume = {438},
year = {2005}
}
@article{Cuvelier2010,
abstract = {Among eukaryotes, four major phytoplankton lineages are responsible for marine photosynthesis; prymnesiophytes, alveolates, stramenopiles, and prasinophytes. Contributions by individual taxa, however, are not well known, and genomes have been analyzed from only the latter two lineages. Tiny "picoplanktonic" members of the prymnesiophyte lineage have long been inferred to be ecologically important but remain poorly characterized. Here, we examine pico-prymnesiophyte evolutionary history and ecology using cultivation-independent methods. 18S rRNA gene analysis showed pico-prymnesiophytes belonged to broadly distributed uncultivated taxa. Therefore, we used targeted metagenomics to analyze uncultured pico-prymnesiophytes sorted by flow cytometry from subtropical North Atlantic waters. The data reveal a composite nuclear-encoded gene repertoire with strong green-lineage affiliations, which contrasts with the evolutionary history indicated by the plastid genome. Measured pico-prymnesiophyte growth rates were rapid in this region, resulting in primary production contributions similar to the cyanobacterium Prochlorococcus. On average, pico-prymnesiophytes formed 25\% of global picophytoplankton biomass, with differing contributions in five biogeographical provinces spanning tropical to subpolar systems. Elements likely contributing to success include high gene density and genes potentially involved in defense and nutrient uptake. Our findings have implications reaching beyond pico-prymnesiophytes, to the prasinophytes and stramenopiles. For example, prevalence of putative Ni-containing superoxide dismutases (SODs), instead of Fe-containing SODs, seems to be a common adaptation among eukaryotic phytoplankton for reducing Fe quotas in low-Fe modern oceans. Moreover, highly mosaic gene repertoires, although compositionally distinct for each major eukaryotic lineage, now seem to be an underlying facet of successful marine phytoplankton.},
author = {Cuvelier, Marie L and Allen, Andrew E and Monier, Adam and McCrow, John P and Messi\'{e}, Monique and Tringe, Susannah G and Woyke, Tanja and Welsh, Rory M and Ishoey, Thomas and Lee, Jae-Hyeok and Binder, Brian J and DuPont, Chris L and Latasa, Mikel and Guigand, C\'{e}dric and Buck, Kurt R and Hilton, Jason and Thiagarajan, Mathangi and Caler, Elisabet and Read, Betsy and Lasken, Roger S and Chavez, Francisco P and Worden, Alexandra Z},
doi = {10.1073/pnas.1001665107},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Amino Acid Sequence,Biomass,Ecosystem,Eukaryota,Eukaryota: classification,Eukaryota: genetics,Eukaryota: growth \& development,Evolution, Molecular,Florida,Geography,Metagenome,Metagenome: genetics,Metagenomics,Metagenomics: methods,Molecular Sequence Data,Oceans and Seas,Phylogeny,Phytoplankton,Phytoplankton: classification,Phytoplankton: genetics,Phytoplankton: growth \& development,RNA, Ribosomal, 16S,RNA, Ribosomal, 16S: genetics,RNA, Ribosomal, 18S,RNA, Ribosomal, 18S: genetics,Seasons,Sequence Homology, Amino Acid,Temperature},
month = aug,
number = {33},
pages = {14679--84},
pmid = {20668244},
title = {{Targeted metagenomics and ecology of globally important uncultured eukaryotic phytoplankton.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930470\&tool=pmcentrez\&rendertype=abstract},
volume = {107},
year = {2010}
}
@article{Daily2010,
abstract = {High-throughput sequencing (HTS) technologies play important roles in the life sciences by allowing the rapid parallel sequencing of very large numbers of relatively short nucleotide sequences, in applications ranging from genome sequencing and resequencing to digital microarrays and ChIP-Seq experiments. As experiments scale up, HTS technologies create new bioinformatics challenges for the storage and sharing of HTS data.},
author = {Daily, Kenny and Rigor, Paul and Christley, Scott and Xie, Xiaohui and Baldi, Pierre},
doi = {10.1186/1471-2105-11-514},
file = {::},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Data Compression,Databases,Entropy,Factual,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods},
month = jan,
number = {1},
pages = {514},
pmid = {20946637},
publisher = {BioMed Central Ltd},
title = {{Data structures and compression algorithms for high-throughput sequencing technologies.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2964686\&tool=pmcentrez\&rendertype=abstract},
volume = {11},
year = {2010}
}
@article{Daniel2004,
abstract = {Soil microorganisms have been the most valuable source of natural products, providing industrially important antibiotics and biocatalysts. But, of late, the discovery rate of novel biomolecules using traditional cultivation techniques has been extremely low, as most soil microorganisms cannot be cultured in this way. The development of novel cultivation-dependent and molecular cultivation-independent approaches has paved the way for a new era of product recovery from soil microorganisms. In particular, gene-mining based on the construction and screening of complex libraries derived from the soil metagenome provides opportunities to fully explore and exploit the enormous genetic and metabolic diversity of soil microorganisms. This strategy has already resulted in the isolation of novel biocatalysts and bioactive molecules.},
author = {Daniel, Rolf},
doi = {10.1016/j.copbio.2004.04.005},
issn = {0958-1669},
journal = {Current opinion in biotechnology},
keywords = {Biotechnology,Biotechnology: methods,Catalysis,Chromosomes, Artificial, Bacterial,DNA,DNA: genetics,Gene Library,Genome,Plasmids,Plasmids: metabolism,Soil,Soil Microbiology},
month = jun,
number = {3},
pages = {199--204},
pmid = {15193327},
title = {{The soil metagenome--a rich resource for the discovery of novel natural products.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15193327},
volume = {15},
year = {2004}
}
@article{Daniel2005,
abstract = {Phylogenetic surveys of soil ecosystems have shown that the number of prokaryotic species found in a single sample exceeds that of known cultured prokaryotes. Soil metagenomics, which comprises isolation of soil DNA and the production and screening of clone libraries, can provide a cultivation-independent assessment of the largely untapped genetic reservoir of soil microbial communities. This approach has already led to the identification of novel biomolecules. However, owing to the complexity and heterogeneity of the biotic and abiotic components of soil ecosystems, the construction and screening of soil-based libraries is difficult and challenging. This review describes how to construct complex libraries from soil samples, and how to use these libraries to unravel functions of soil microbial communities.},
author = {Daniel, Rolf},
doi = {10.1038/nrmicro1160},
issn = {1740-1526},
journal = {Nature reviews. Microbiology},
keywords = {DNA,DNA: genetics,Gene Library,Genomics,Soil Microbiology},
month = jun,
number = {6},
pages = {470--8},
pmid = {15931165},
title = {{The metagenomics of soil.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15931165},
volume = {3},
year = {2005}
}
@article{Deorowicz2011,
abstract = {Modern sequencing instruments are able to generate at least hundreds of millions short reads of genomic data. Those huge volumes of data require effective means to store them, provide quick access to any record and enable fast decompression.},
author = {Deorowicz, Sebastian and Grabowski, Szymon},
doi = {10.1093/bioinformatics/btr014},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Base Sequence,Computational Biology,Computational Biology: methods,Data Compression,Data Compression: methods,Genomics,Internet,Sequence Analysis, DNA},
month = mar,
number = {6},
pages = {860--2},
pmid = {21252073},
title = {{Compression of DNA sequence reads in FASTQ format.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21252073},
volume = {27},
year = {2011}
}
@article{Flicek2010,
author = {Flicek, Paul and Birney, Ewan},
doi = {10.1038/NmEtH.1376},
file = {::},
journal = {Nature Methods},
number = {11s},
pages = {S6--S12},
title = {{Sense from sequence reads : methods for alignment and assembly}},
volume = {6},
year = {2009}
}
@article{Hess2011,
abstract = {The paucity of enzymes that efficiently deconstruct plant polysaccharides represents a major bottleneck for industrial-scale conversion of cellulosic biomass into biofuels. Cow rumen microbes specialize in degradation of cellulosic plant material, but most members of this complex community resist cultivation. To characterize biomass-degrading genes and genomes, we sequenced and analyzed 268 gigabases of metagenomic DNA from microbes adherent to plant fiber incubated in cow rumen. From these data, we identified 27,755 putative carbohydrate-active genes and expressed 90 candidate proteins, of which 57\% were enzymatically active against cellulosic substrates. We also assembled 15 uncultured microbial genomes, which were validated by complementary methods including single-cell genome sequencing. These data sets provide a substantially expanded catalog of genes and genomes participating in the deconstruction of cellulosic biomass.},
author = {Hess, Matthias and Sczyrba, Alexander and Egan, Rob and Kim, Tae-Wan and Chokhawala, Harshal and Schroth, Gary and Luo, Shujun and Clark, Douglas S and Chen, Feng and Zhang, Tao and Mackie, Roderick I and Pennacchio, Len a and Tringe, Susannah G and Visel, Axel and Woyke, Tanja and Wang, Zhong and Rubin, Edward M},
doi = {10.1126/science.1200387},
file = {::},
issn = {1095-9203},
journal = {Science},
keywords = {Amino Acid Sequence,Animals,Bacteria,Bacteria: enzymology,Bacteria: genetics,Bacteria: isolation \& purification,Bacteria: metabolism,Bacterial,Bacterial Proteins,Bacterial Proteins: chemistry,Bacterial Proteins: genetics,Bacterial Proteins: metabolism,Biomass,Carbohydrate Metabolism,Cattle,Cattle: microbiology,Cellulase,Cellulase: genetics,Cellulase: metabolism,Cellulases,Cellulases: chemistry,Cellulases: genetics,Cellulases: metabolism,Cellulose,Cellulose 1,4-beta-Cellobiosidase,Cellulose 1,4-beta-Cellobiosidase: genetics,Cellulose 1,4-beta-Cellobiosidase: metabolism,Cellulose: metabolism,DNA,Genes, Bacterial,Genome, Bacterial,Metagenome,Metagenomics,Metagenomics: methods,Molecular Sequence Annotation,Molecular Sequence Data,Poaceae,Poaceae: microbiology,Rumen,Rumen: metabolism,Rumen: microbiology,Sequence Analysis, DNA},
month = jan,
number = {6016},
pages = {463--7},
pmid = {21273488},
title = {{Metagenomic discovery of biomass-degrading genes and genomes from cow rumen.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21273488},
volume = {331},
year = {2011}
}
@article{Hsi-YangFritz2011,
abstract = {Data storage costs have become an appreciable proportion of total cost in the creation and analysis of DNA sequence data. Of particular concern is that the rate of increase in DNA sequencing is significantly outstripping the rate of increase in disk storage capacity. In this paper we present a new reference-based compression method that efficiently compresses DNA sequences for storage. Our approach works for resequencing experiments that target well-studied genomes. We align new sequences to a reference genome and then encode the differences between the new sequence and the reference genome for storage. Our compression method is most efficient when we allow controlled loss of data in the saving of quality information and unaligned sequences. With this new compression method we observe exponential efficiency gains as read lengths increase, and the magnitude of this efficiency gain can be controlled by changing the amount of quality information stored. Our compression method is tunable: The storage of quality scores and unaligned sequences may be adjusted for different experiments to conserve information or to minimize storage costs, and provides one opportunity to address the threat that increasing DNA sequence volumes will overcome our ability to store the sequences.},
author = {{Hsi-Yang Fritz}, Markus and Leinonen, Rasko and Cochrane, Guy and Birney, Ewan},
doi = {10.1101/gr.114819.110},
file = {::},
issn = {1549-5469},
journal = {Genome Research},
month = may,
number = {5},
pages = {734--40},
pmid = {21245279},
title = {{Efficient storage of high throughput DNA sequencing data using reference-based compression.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3083090\&tool=pmcentrez\&rendertype=abstract},
volume = {21},
year = {2011}
}
@article{Kakirde2010,
abstract = {Metagenomic analyses can provide extensive information on the structure, composition, and predicted gene functions of diverse environmental microbial assemblages. Each environment presents its own unique challenges to metagenomic investigation and requires a specifically designed approach to accommodate physicochemical and biotic factors unique to each environment that can pose technical hurdles and/or bias the metagenomic analyses. In particular, soils harbor an exceptional diversity of prokaryotes that are largely undescribed beyond the level of ribotype and are a potentially vast resource for natural product discovery. The successful application of a soil metagenomic approach depends on selecting the appropriate DNA extraction, purification, and if necessary, cloning methods for the intended downstream analyses. The most important technical considerations in a metagenomic study include obtaining a sufficient yield of high-purity DNA representing the targeted microorganisms within an environmental sample or enrichment and (if required) constructing a metagenomic library in a suitable vector and host. Size does matter in the context of the average insert size within a clone library or the sequence read length for a high-throughput sequencing approach. It is also imperative to select the appropriate metagenomic screening strategy to address the specific question(s) of interest, which should drive the selection of methods used in the earlier stages of a metagenomic project (e.g., DNA size, to clone or not to clone). Here, we present both the promising and problematic nature of soil metagenomics and discuss the factors that should be considered when selecting soil sampling, DNA extraction, purification, and cloning methods to implement based on the ultimate study objectives.},
author = {Kakirde, Kavita S and Parsley, Larissa C and Liles, Mark R},
doi = {10.1016/j.soilbio.2010.07.021},
issn = {0038-0717},
journal = {Soil biology \& biochemistry},
month = nov,
number = {11},
pages = {1911--1923},
pmid = {21076656},
publisher = {Elsevier Ltd},
title = {{Size Does Matter: Application-driven Approaches for Soil Metagenomics.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2976544\&tool=pmcentrez\&rendertype=abstract},
volume = {42},
year = {2010}
}
@article{Konstantinidis2009,
abstract = {The deep sea (water depth of >2,000 m) represents the largest biome on Earth. Yet relatively little is known about its microbial community's structure, function, and adaptation to the cold and deep biosphere. To provide further genomic insights into deep-sea planktonic microbes, we sequenced a total of approximately 200 Mbp of a random whole-genome shotgun (WGS) library from a microbial community residing at a depth of 4,000 m at Station ALOHA in the Pacific Ocean and compared it to other available WGS sequence data from surface and deep waters. Our analyses indicated that the deep-sea lifestyle is likely facilitated by a collection of very subtle adaptations, as opposed to dramatic alterations of gene content or structure. These adaptations appear to include higher metabolic versatility and genomic plasticity to cope with the sparse and sporadic energy resources available, a preference for hydrophobic and smaller-volume amino acids in protein sequences, unique proteins not found in surface-dwelling species, and adaptations at the gene expression level. The deep-sea community is also characterized by a larger average genome size and a higher content of "selfish" genetic elements, such as transposases and prophages, whose propagation is apparently favored by more relaxed purifying (negative) selection in deeper waters.},
author = {Konstantinidis, Konstantinos T and Braff, Jennifer and Karl, David M and DeLong, Edward F},
doi = {10.1128/AEM.00473-09},
file = {::},
issn = {1098-5336},
journal = {Applied and environmental microbiology},
keywords = {Archaea,Archaea: classification,Archaea: genetics,Archaeal Proteins,Archaeal Proteins: genetics,Bacteria,Bacteria: classification,Bacteria: genetics,Bacterial Proteins,Bacterial Proteins: genetics,Computational Biology,Ecosystem,Genomic Library,Genomics,Hawaii,Metabolic Networks and Pathways,Molecular Sequence Data,Pacific Ocean,Plankton,Plankton: classification,Plankton: genetics,Seawater,Seawater: microbiology,Sequence Analysis, DNA},
month = aug,
number = {16},
pages = {5345--55},
pmid = {19542347},
title = {{Comparative metagenomic analysis of a microbial community residing at a depth of 4,000 meters at station ALOHA in the North Pacific subtropical gyre.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2725473\&tool=pmcentrez\&rendertype=abstract},
volume = {75},
year = {2009}
}
@article{Kozanitis2011,
abstract = {With the advent of next generation sequencing technologies, the cost of sequencing whole genomes is poised to go below \$1000 per human individual in a few years. As more and more genomes are sequenced, analysis methods are undergoing rapid development, making it tempting to store sequencing data for long periods of time so that the data can be re-analyzed with the latest techniques. The challenging open research problems, huge influx of data, and rapidly improving analysis techniques have created the need to store and transfer very large volumes of data. Compression can be achieved at many levels, including trace level (compressing image data), sequence level (compressing a genomic sequence), and fragment-level (compressing a set of short, redundant fragment reads, along with quality-values on the base-calls). We focus on fragment-level compression, which is the pressing need today. Our article makes two contributions, implemented in a tool, SlimGene. First, we introduce a set of domain specific loss-less compression schemes that achieve over 40× compression of fragments, outperforming bzip2 by over 6×. Including quality values, we show a 5× compression using less running time than bzip2. Second, given the discrepancy between the compression factor obtained with and without quality values, we initiate the study of using "lossy" quality values. Specifically, we show that a lossy quality value quantization results in 14× compression but has minimal impact on downstream applications like SNP calling that use the quality values. Discrepancies between SNP calls made between the lossy and loss-less versions of the data are limited to low coverage areas where even the SNP calls made by the loss-less version are marginal.},
author = {Kozanitis, Christos and Saunders, Chris and Kruglyak, Semyon and Bafna, Vineet and Varghese, George},
doi = {10.1089/cmb.2010.0253},
file = {::},
issn = {1557-8666},
journal = {Journal of Computational Biology : a Journal of Computational Molecular Cell Biology},
keywords = {Algorithms,Data Compression,Data Compression: methods,Genome, Human,Genomics,Genomics: methods,Humans,Polymorphism, Single Nucleotide,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
month = mar,
number = {3},
pages = {401--13},
pmid = {21385043},
title = {{Compressing genomic sequence fragments using SlimGene.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3123913\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2011}
}
@article{Lander2001,
abstract = {The human genome holds an extraordinary trove of information about human development, physiology, medicine and evolution. Here we report the results of an international collaboration to produce and make freely available a draft sequence of the human genome. We also present an initial analysis of the data, describing some of the insights that can be gleaned from the sequence.},
author = {Lander, E S and Linton, L M and Birren, B and Nusbaum, C and Zody, M C and Baldwin, J and Devon, K and Dewar, K and Doyle, M and FitzHugh, W and Funke, R and Gage, D and Harris, K and Heaford, a and Howland, J and Kann, L and Lehoczky, J and LeVine, R and McEwan, P and McKernan, K and Meldrim, J and Mesirov, J P and Miranda, C and Morris, W and Naylor, J and Raymond, C and Rosetti, M and Santos, R and Sheridan, a and Sougnez, C and Stange-Thomann, N and Stojanovic, N and Subramanian, a and Wyman, D and Rogers, J and Sulston, J and Ainscough, R and Beck, S and Bentley, D and Burton, J and Clee, C and Carter, N and Coulson, a and Deadman, R and Deloukas, P and Dunham, a and Dunham, I and Durbin, R and French, L and Grafham, D and Gregory, S and Hubbard, T and Humphray, S and Hunt, a and Jones, M and Lloyd, C and McMurray, a and Matthews, L and Mercer, S and Milne, S and Mullikin, J C and Mungall, a and Plumb, R and Ross, M and Shownkeen, R and Sims, S and Waterston, R H and Wilson, R K and Hillier, L W and McPherson, J D and Marra, M a and Mardis, E R and Fulton, L a and Chinwalla, a T and Pepin, K H and Gish, W R and Chissoe, S L and Wendl, M C and Delehaunty, K D and Miner, T L and Delehaunty, a and Kramer, J B and Cook, L L and Fulton, R S and Johnson, D L and Minx, P J and Clifton, S W and Hawkins, T and Branscomb, E and Predki, P and Richardson, P and Wenning, S and Slezak, T and Doggett, N and Cheng, J F and Olsen, a and Lucas, S and Elkin, C and Uberbacher, E and Frazier, M and Gibbs, R a and Muzny, D M and Scherer, S E and Bouck, J B and Sodergren, E J and Worley, K C and Rives, C M and Gorrell, J H and Metzker, M L and Naylor, S L and Kucherlapati, R S and Nelson, D L and Weinstock, G M and Sakaki, Y and Fujiyama, a and Hattori, M and Yada, T and Toyoda, a and Itoh, T and Kawagoe, C and Watanabe, H and Totoki, Y and Taylor, T and Weissenbach, J and Heilig, R and Saurin, W and Artiguenave, F and Brottier, P and Bruls, T and Pelletier, E and Robert, C and Wincker, P and Smith, D R and Doucette-Stamm, L and Rubenfield, M and Weinstock, K and Lee, H M and Dubois, J and Rosenthal, a and Platzer, M and Nyakatura, G and Taudien, S and Rump, a and Yang, H and Yu, J and Wang, J and Huang, G and Gu, J and Hood, L and Rowen, L and Madan, a and Qin, S and Davis, R W and Federspiel, N a and Abola, a P and Proctor, M J and Myers, R M and Schmutz, J and Dickson, M and Grimwood, J and Cox, D R and Olson, M V and Kaul, R and Shimizu, N and Kawasaki, K and Minoshima, S and Evans, G a and Athanasiou, M and Schultz, R and Roe, B a and Chen, F and Pan, H and Ramser, J and Lehrach, H and Reinhardt, R and McCombie, W R and de la Bastide, M and Dedhia, N and Bl\"{o}cker, H and Hornischer, K and Nordsiek, G and Agarwala, R and Aravind, L and Bailey, J a and Bateman, a and Batzoglou, S and Birney, E and Bork, P and Brown, D G and Burge, C B and Cerutti, L and Chen, H C and Church, D and Clamp, M and Copley, R R and Doerks, T and Eddy, S R and Eichler, E E and Furey, T S and Galagan, J and Gilbert, J G and Harmon, C and Hayashizaki, Y and Haussler, D and Hermjakob, H and Hokamp, K and Jang, W and Johnson, L S and Jones, T a and Kasif, S and Kaspryzk, a and Kennedy, S and Kent, W J and Kitts, P and Koonin, E V and Korf, I and Kulp, D and Lancet, D and Lowe, T M and McLysaght, a and Mikkelsen, T and Moran, J V and Mulder, N and Pollara, V J and Ponting, C P and Schuler, G and Schultz, J and Slater, G and Smit, a F and Stupka, E and Szustakowski, J and Thierry-Mieg, D and Thierry-Mieg, J and Wagner, L and Wallis, J and Wheeler, R and Williams, a and Wolf, Y I and Wolfe, K H and Yang, S P and Yeh, R F and Collins, F and Guyer, M S and Peterson, J and Felsenfeld, a and Wetterstrand, K a and Patrinos, a and Morgan, M J and de Jong, P and Catanese, J J and Osoegawa, K and Shizuya, H and Choi, S and Chen, Y J and Szustakowki, J},
doi = {10.1038/35057062},
file = {::},
issn = {0028-0836},
journal = {Nature},
keywords = {Animals,Chromosome Mapping,Conserved Sequence,CpG Islands,DNA Transposable Elements,Databases, Factual,Drug Industry,Evolution, Molecular,Forecasting,GC Rich Sequence,Gene Duplication,Genes,Genetic Diseases, Inborn,Genetics, Medical,Genome, Human,Human Genome Project,Humans,Mutation,Private Sector,Proteins,Proteins: genetics,Proteome,Public Sector,RNA,RNA: genetics,Repetitive Sequences, Nucleic Acid,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Species Specificity},
month = feb,
number = {6822},
pages = {860--921},
pmid = {11237011},
title = {{Initial sequencing and analysis of the human genome.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11237011},
volume = {409},
year = {2001}
}
@article{Langmead2009,
abstract = {Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds. Bowtie is open source (http://bowtie.cbcb.umd.edu).},
author = {Langmead, Ben and Trapnell, Cole and Pop, Mihai and Salzberg, Steven L},
doi = {10.1186/gb-2009-10-3-r25},
file = {::},
issn = {1465-6914},
journal = {Genome Biology},
keywords = {Algorithms,Base Sequence,Genome,Human,Human: genetics,Humans,Sequence Alignment,Sequence Alignment: methods},
month = jan,
number = {3},
pages = {R25},
pmid = {19261174},
title = {{Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2690996\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Leinonen2011,
abstract = {The combination of significantly lower cost and increased speed of sequencing has resulted in an explosive growth of data submitted into the primary next-generation sequence data archive, the Sequence Read Archive (SRA). The preservation of experimental data is an important part of the scientific record, and increasing numbers of journals and funding agencies require that next-generation sequence data are deposited into the SRA. The SRA was established as a public repository for the next-generation sequence data and is operated by the International Nucleotide Sequence Database Collaboration (INSDC). INSDC partners include the National Center for Biotechnology Information (NCBI), the European Bioinformatics Institute (EBI) and the DNA Data Bank of Japan (DDBJ). The SRA is accessible at http://www.ncbi.nlm.nih.gov/Traces/sra from NCBI, at http://www.ebi.ac.uk/ena from EBI and at http://trace.ddbj.nig.ac.jp from DDBJ. In this article, we present the content and structure of the SRA, detail our support for sequencing platforms and provide recommended data submission levels and formats. We also briefly outline our response to the challenge of data growth.},
author = {Leinonen, Rasko and Sugawara, Hideaki and Shumway, Martin},
doi = {10.1093/nar/gkq1019},
file = {::},
issn = {1362-4962},
journal = {Nucleic Acids Research},
keywords = {Databases,High-Throughput Nucleotide Sequencing,Nucleic Acid},
month = jan,
number = {Database issue},
pages = {D19--21},
pmid = {21062823},
title = {{The sequence read archive}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3013647\&tool=pmcentrez\&rendertype=abstract},
volume = {39},
year = {2011}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics},
keywords = {Algorithms,Base Sequence,Computational Biology,Computational Biology: methods,DNA,DNA: methods,Genome,Genomics,Molecular Sequence Data,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software},
month = aug,
number = {16},
pages = {2078--9},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2723002\&tool=pmcentrez\&rendertype=abstract},
volume = {25},
year = {2009}
}
@article{Li2009,
abstract = {SUMMARY: SOAP2 is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate. We used a Burrows Wheeler Transformation (BWT) compression index to substitute the seed strategy for indexing the reference sequence in the main memory. We tested it on the whole human genome and found that this new algorithm reduced memory usage from 14.7 to 5.4 GB and improved alignment speed by 20-30 times. SOAP2 is compatible with both single- and paired-end reads. Additionally, this tool now supports multiple text and compressed file formats. A consensus builder has also been developed for consensus assembly and SNP detection from alignment of short reads on a reference genome. AVAILABILITY: http://soap.genomics.org.cn.},
author = {Li, Ruiqiang and Yu, Chang and Li, Yingrui and Lam, Tak-Wah and Yiu, Siu-Ming and Kristiansen, Karsten and Wang, Jun},
doi = {10.1093/bioinformatics/btp336},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Computational Biology,Computational Biology: methods,Genome, Human,Humans,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = aug,
number = {15},
pages = {1966--7},
pmid = {19497933},
title = {{SOAP2: an improved ultrafast tool for short read alignment.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19497933},
volume = {25},
year = {2009}
}
@article{Li2010,
abstract = {Next-generation massively parallel DNA sequencing technologies provide ultrahigh throughput at a substantially lower unit data cost; however, the data are very short read length sequences, making de novo assembly extremely challenging. Here, we describe a novel method for de novo assembly of large genomes from short read sequences. We successfully assembled both the Asian and African human genome sequences, achieving an N50 contig size of 7.4 and 5.9 kilobases (kb) and scaffold of 446.3 and 61.9 kb, respectively. The development of this de novo short read assembly method creates new opportunities for building reference sequences and carrying out accurate analyses of unexplored genomes in a cost-effective way.},
author = {Li, Ruiqiang and Zhu, Hongmei and Ruan, Jue and Qian, Wubin and Fang, Xiaodong and Shi, Zhongbin and Li, Yingrui and Li, Shengting and Shan, Gao and Kristiansen, Karsten and Li, Songgang and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {10.1101/gr.097261.109},
file = {::},
isbn = {8675525274247},
issn = {1549-5469},
journal = {Genome research},
keywords = {African Continental Ancestry Group,African Continental Ancestry Group: genetics,Asian Continental Ancestry Group,Asian Continental Ancestry Group: genetics,Genome, Human,Human Genome Project,Humans,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: economics,Oligonucleotide Array Sequence Analysis: methods,Sequence Alignment,Sequence Alignment: economics,Sequence Alignment: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: economics,Sequence Analysis, DNA: methods},
month = feb,
number = {2},
pages = {265--72},
pmid = {20019144},
title = {{De novo assembly of human genomes with massively parallel short read sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2813482\&tool=pmcentrez\&rendertype=abstract},
volume = {20},
year = {2010}
}
@article{Li2009a,
abstract = {The remarkable advance of metagenomics presents significant new challenges in data analysis. Metagenomic datasets (metagenomes) are large collections of sequencing reads from anonymous species within particular environments. Computational analyses for very large metagenomes are extremely time-consuming, and there are often many novel sequences in these metagenomes that are not fully utilized. The number of available metagenomes is rapidly increasing, so fast and efficient metagenome comparison methods are in great demand.},
author = {Li, Weizhong},
doi = {10.1186/1471-2105-10-359},
file = {::},
isbn = {1471210510},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Cluster Analysis,Computational Biology,Computational Biology: methods,Metagenomics,Metagenomics: methods,Pattern Recognition, Automated,Sequence Alignment,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
month = jan,
pages = {359},
pmid = {19863816},
title = {{Analysis and comparison of very large metagenomes with fast clustering and functional annotation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2774329\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Mardis2008,
abstract = {Recent scientific discoveries that resulted from the application of next-generation DNA sequencing technologies highlight the striking impact of these massively parallel platforms on genetics. These new methods have expanded previously focused readouts from a variety of DNA preparation protocols to a genome-wide scale and have fine-tuned their resolution to single base precision. The sequencing of RNA also has transitioned and now includes full-length cDNA analyses, serial analysis of gene expression (SAGE)-based methods, and noncoding RNA discovery. Next-generation sequencing has also enabled novel applications such as the sequencing of ancient DNA samples, and has substantially widened the scope of metagenomic analysis of environmentally derived samples. Taken together, an astounding potential exists for these technologies to bring enormous change in genetic and biological research and to enhance our fundamental biological knowledge.},
author = {Mardis, Elaine R},
doi = {10.1146/annurev.genom.9.081307.164359},
file = {::},
issn = {1527-8204},
journal = {Annual review of genomics and human genetics},
keywords = {Chromatin Immunoprecipitation,Chromatin Immunoprecipitation: methods,Fossils,Gene Expression Profiling,Gene Expression Profiling: methods,Genome, Human,Genomics,Genomics: methods,Humans,RNA, Untranslated,RNA, Untranslated: genetics,Sequence Analysis, DNA,Sequence Analysis, DNA: instrumentation,Sequence Analysis, DNA: methods,Sequence Analysis, DNA: trends},
month = jan,
pages = {387--402},
pmid = {18576944},
title = {{Next-generation DNA sequencing methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18576944},
volume = {9},
year = {2008}
}
@article{Maxam1992,
author = {Maxam, a M and Gilbert, W},
file = {::},
issn = {0740-7378},
journal = {Biotechnology (Reading, Mass.)},
keywords = {Base Sequence,History, 20th Century,Molecular Sequence Data},
month = jan,
number = {2},
pages = {99--103},
pmid = {1422074},
title = {{A new method for sequencing DNA. 1977.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1422074},
volume = {24},
year = {1992}
}
@article{Mccarren2010,
author = {Mccarren, Jay and Becker, Jamie W and Repeta, Daniel J and Shi, Yanmei and Young, Curtis R and Malmstrom, Rex R},
doi = {10.1073/pnas.1010732107/-/DCSupplemental.www.pnas.org/cgi/doi/10.1073/pnas.1010732107},
file = {::},
journal = {PNAS},
title = {{Microbial community transcriptomes reveal microbes and metabolic pathways associated with dissolved organic matter turnover in the sea}},
year = {2010}
}
@article{O'Connor2010,
abstract = {Since the introduction of next-generation DNA sequencers the rapid increase in sequencer throughput, and associated drop in costs, has resulted in more than a dozen human genomes being resequenced over the last few years. These efforts are merely a prelude for a future in which genome resequencing will be commonplace for both biomedical research and clinical applications. The dramatic increase in sequencer output strains all facets of computational infrastructure, especially databases and query interfaces. The advent of cloud computing, and a variety of powerful tools designed to process petascale datasets, provide a compelling solution to these ever increasing demands.},
author = {O'Connor, Brian D and Merriman, Barry and Nelson, Stanley F},
doi = {10.1186/1471-2105-11-S12-S2},
file = {::},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {DNA,DNA: methods,Databases,Genome,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,Human,Humans,Nucleic Acid,Sequence Analysis,Software},
month = jan,
number = {Suppl 12},
pages = {S2},
pmid = {21210981},
title = {{SeqWare Query Engine: storing and searching sequence data in the cloud.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3040528\&tool=pmcentrez\&rendertype=abstract},
volume = {11 Suppl 1},
year = {2010}
}
@article{Paszkiewicz2010,
abstract = {A new generation of sequencing technologies is revolutionizing molecular biology. Illumina's Solexa and Applied Biosystems' SOLiD generate gigabases of nucleotide sequence per week. However, a perceived limitation of these ultra-high-throughput technologies is their short read-lengths. De novo assembly of sequence reads generated by classical Sanger capillary sequencing is a mature field of research. Unfortunately, the existing sequence assembly programs were not effective for short sequence reads generated by Illumina and SOLiD platforms. Early studies suggested that, in principle, sequence reads as short as 20-30 nucleotides could be used to generate useful assemblies of both prokaryotic and eukaryotic genome sequences, albeit containing many gaps. The early feasibility studies and proofs of principle inspired several bioinformatics research groups to implement new algorithms as freely available software tools specifically aimed at assembling reads of 30-50 nucleotides in length. This has led to the generation of several draft genome sequences based exclusively on short sequence Illumina sequence reads, recently culminating in the assembly of the 2.25-Gb genome of the giant panda from Illumina sequence reads with an average length of just 52 nucleotides. As well as reviewing recent developments in the field, we discuss some practical aspects such as data filtering and submission of assembly data to public repositories.},
author = {Paszkiewicz, Konrad and Studholme, David J},
doi = {10.1093/bib/bbq020},
file = {::},
issn = {1477-4054},
journal = {Briefings in bioinformatics},
keywords = {Algorithms,Animals,Base Sequence,Computational Biology,Computational Biology: methods,Databases, Genetic,Genome,Humans,Molecular Sequence Data,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
month = sep,
number = {5},
pages = {457--72},
pmid = {20724458},
title = {{De novo assembly of short sequence reads.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20724458},
volume = {11},
year = {2010}
}
@article{Pevzner2001a,
abstract = {For the last twenty years fragment assembly was dominated by the "overlap - layout - consensus" algorithms that are used in all currently available assembly tools. However, the limits of these algorithms are being tested in the era of genomic sequencing and it is not clear whether they are the best choice for large-scale assemblies. Although the "overlap - layout - consensus" approach proved to be useful in assembling clones, it faces difficulties in genomic assemblies: the existing algorithms make assembly errors even in bacterial genomes. We abandoned the "overlap - layout - consensus" approach in favour of a new Eulerian Superpath approach that outperforms the existing algorithms for genomic fragment assembly (Pevzner et al. 2001 InProceedings of the Fifth Annual International Conference on Computational Molecular Biology (RECOMB-01), 256-26). In this paper we describe our new EULER-DB algorithm that, similarly to the Celera assembler takes advantage of clone-end sequencing by using the double-barreled data. However, in contrast to the Celera assembler, EULER-DB does not mask repeats but uses them instead as a powerful tool for contig ordering. We also describe a new approach for the Copy Number Problem: "How many times a given repeat is present in the genome?". For long nearly-perfect repeats this question is notoriously difficult and some copies of such repeats may be "lost" in genomic assemblies. We describe our EULER-CN algorithm for the Copy Number Problem that proved to be successful in difficult sequencing projects.},
author = {Pevzner, P. A. and Tang, H.},
file = {::},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Computational Biology,DNA,DNA: statistics \& numerical data,Genetic,Genetic Techniques,Genetic Techniques: statistics \& numerical data,Genome,Models,Nucleic Acid Hybridization,Sequence Analysis},
month = jan,
number = {Supplement 1},
pages = {S225--S233},
pmid = {11473013},
title = {{Fragment assembly with double-barreled data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11473013},
volume = {17},
year = {2001}
}
@article{Pevzner2001,
abstract = {For the last 20 years, fragment assembly in DNA sequencing followed the "overlap-layout-consensus" paradigm that is used in all currently available assembly tools. Although this approach proved useful in assembling clones, it faces difficulties in genomic shotgun assembly. We abandon the classical "overlap-layout-consensus" approach in favor of a new euler algorithm that, for the first time, resolves the 20-year-old "repeat problem" in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem that allows one to generate accurate solutions of large-scale sequencing problems. euler, in contrast to the celera assembler, does not mask such repeats but uses them instead as a powerful fragment assembly tool.},
author = {Pevzner, P. A. and Tang, H. and Waterman, M. S.},
doi = {10.1073/pnas.171285098},
file = {::},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Campylobacter jejuni,Campylobacter jejuni: genetics,Contig Mapping,Contig Mapping: methods,DNA, Bacterial,DNA, Bacterial: genetics,Genome, Bacterial,Lactococcus lactis,Lactococcus lactis: genetics,Models, Theoretical,Neisseria meningitidis,Neisseria meningitidis: genetics,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = aug,
number = {17},
pages = {9748--53},
pmid = {11504945},
title = {{An Eulerian path approach to DNA fragment assembly.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=55524\&tool=pmcentrez\&rendertype=abstract},
volume = {98},
year = {2001}
}
@article{Qin2010,
abstract = {To understand the impact of gut microbes on human health and well-being it is crucial to assess their genetic potential. Here we describe the Illumina-based metagenomic sequencing, assembly and characterization of 3.3 million non-redundant microbial genes, derived from 576.7 gigabases of sequence, from faecal samples of 124 European individuals. The gene set, approximately 150 times larger than the human gene complement, contains an overwhelming majority of the prevalent (more frequent) microbial genes of the cohort and probably includes a large proportion of the prevalent human intestinal microbial genes. The genes are largely shared among individuals of the cohort. Over 99\% of the genes are bacterial, indicating that the entire cohort harbours between 1,000 and 1,150 prevalent bacterial species and each individual at least 160 such species, which are also largely shared. We define and describe the minimal gut metagenome and the minimal gut bacterial genome in terms of functions present in all individuals and most bacteria, respectively.},
author = {Qin, Junjie and Li, Ruiqiang and Raes, Jeroen and Arumugam, Manimozhiyan and Burgdorf, Kristoffer Solvsten and Manichanh, Chaysavanh and Nielsen, Trine and Pons, Nicolas and Levenez, Florence and Yamada, Takuji and Mende, Daniel R and Li, Junhua and Xu, Junming and Li, Shaochuan and Li, Dongfang and Cao, Jianjun and Wang, Bo and Liang, Huiqing and Zheng, Huisong and Xie, Yinlong and Tap, Julien and Lepage, Patricia and Bertalan, Marcelo and Batto, Jean-Michel and Hansen, Torben and {Le Paslier}, Denis and Linneberg, Allan and Nielsen, H Bj\o rn and Pelletier, Eric and Renault, Pierre and Sicheritz-Ponten, Thomas and Turner, Keith and Zhu, Hongmei and Yu, Chang and Li, Shengting and Jian, Min and Zhou, Yan and Li, Yingrui and Zhang, Xiuqing and Li, Songgang and Qin, Nan and Yang, Huanming and Wang, Jian and Brunak, S\o ren and Dor\'{e}, Joel and Guarner, Francisco and Kristiansen, Karsten and Pedersen, Oluf and Parkhill, Julian and Weissenbach, Jean and Bork, Peer and Ehrlich, S Dusko and Wang, Jun},
doi = {10.1038/nature08821},
file = {::},
issn = {1476-4687},
journal = {Nature},
keywords = {Adult,Bacteria,Bacteria: classification,Bacteria: genetics,Bacteria: isolation \& purification,Bacteria: metabolism,Cohort Studies,Contig Mapping,Denmark,Feces,Feces: microbiology,Gastrointestinal Tract,Gastrointestinal Tract: microbiology,Genes, Bacterial,Genes, Bacterial: genetics,Genes, Essential,Genes, Essential: genetics,Genome, Bacterial,Genome, Bacterial: genetics,Genomics,Health,Humans,Inflammatory Bowel Diseases,Inflammatory Bowel Diseases: genetics,Metagenome,Metagenome: genetics,Obesity,Obesity: genetics,Open Reading Frames,Open Reading Frames: genetics,Overweight,Overweight: genetics,Sequence Analysis, DNA,Spain},
month = mar,
number = {7285},
pages = {59--65},
pmid = {20203603},
title = {{A human gut microbial gene catalogue established by metagenomic sequencing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20203603},
volume = {464},
year = {2010}
}
@article{Raes2011,
abstract = {Using metagenomic 'parts lists' to infer global patterns on microbial ecology remains a significant challenge. To deduce important ecological indicators such as environmental adaptation, molecular trait dispersal, diversity variation and primary production from the gene pool of an ecosystem, we integrated 25 ocean metagenomes with geographical, meteorological and geophysicochemical data. We find that climatic factors (temperature, sunlight) are the major determinants of the biomolecular repertoire of each sample and the main limiting factor on functional trait dispersal (absence of biogeographic provincialism). Molecular functional richness and diversity show a distinct latitudinal gradient peaking at 20° N and correlate with primary production. The latter can also be predicted from the molecular functional composition of an environmental sample. Together, our results show that the functional community composition derived from metagenomes is an important quantitative readout for molecular trait-based biogeography and ecology.},
author = {Raes, Jeroen and Letunic, Ivica and Yamada, Takuji and Jensen, Lars Juhl and Bork, Peer},
doi = {10.1038/msb.2011.6},
file = {::},
issn = {1744-4292},
journal = {Molecular systems biology},
keywords = {Adaptation, Physiological,Algorithms,Biodiversity,Climate,Data Interpretation, Statistical,Ecology,Ecosystem,Genetic Loci,Genetic Loci: genetics,Geography,Metabolic Networks and Pathways,Metagenomics,Molecular Sequence Annotation,Oceans and Seas,Regression Analysis,Seawater,Seawater: microbiology,Species Specificity},
month = mar,
number = {473},
pages = {473},
pmid = {21407210},
publisher = {Nature Publishing Group},
title = {{Toward molecular trait-based ecology through integration of biogeochemical, geographical and metagenomic data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3094067\&tool=pmcentrez\&rendertype=abstract},
volume = {7},
year = {2011}
}
@article{Rinta-Kanto2011,
author = {Rinta-Kanto, Johanna M. and Sun, Shulei and Sharma, Shalabh and Kiene, Ronald P. and Moran, Mary Ann},
doi = {10.1111/j.1462-2920.2011.02602.x},
file = {::},
issn = {14622912},
journal = {Environmental Microbiology},
month = nov,
pages = {no--no},
title = {{Bacterial community transcription patterns during a marine phytoplankton bloom}},
url = {http://doi.wiley.com/10.1111/j.1462-2920.2011.02602.x},
volume = {2602},
year = {2011}
}
@article{Rusch2007,
abstract = {The world's oceans contain a complex mixture of micro-organisms that are for the most part, uncharacterized both genetically and biochemically. We report here a metagenomic study of the marine planktonic microbiota in which surface (mostly marine) water samples were analyzed as part of the Sorcerer II Global Ocean Sampling expedition. These samples, collected across a several-thousand km transect from the North Atlantic through the Panama Canal and ending in the South Pacific yielded an extensive dataset consisting of 7.7 million sequencing reads (6.3 billion bp). Though a few major microbial clades dominate the planktonic marine niche, the dataset contains great diversity with 85\% of the assembled sequence and 57\% of the unassembled data being unique at a 98\% sequence identity cutoff. Using the metadata associated with each sample and sequencing library, we developed new comparative genomic and assembly methods. One comparative genomic method, termed "fragment recruitment," addressed questions of genome structure, evolution, and taxonomic or phylogenetic diversity, as well as the biochemical diversity of genes and gene families. A second method, termed "extreme assembly," made possible the assembly and reconstruction of large segments of abundant but clearly nonclonal organisms. Within all abundant populations analyzed, we found extensive intra-ribotype diversity in several forms: (1) extensive sequence variation within orthologous regions throughout a given genome; despite coverage of individual ribotypes approaching 500-fold, most individual sequencing reads are unique; (2) numerous changes in gene content some with direct adaptive implications; and (3) hypervariable genomic islands that are too variable to assemble. The intra-ribotype diversity is organized into genetically isolated populations that have overlapping but independent distributions, implying distinct environmental preference. We present novel methods for measuring the genomic similarity between metagenomic samples and show how they may be grouped into several community types. Specific functional adaptations can be identified both within individual ribotypes and across the entire community, including proteorhodopsin spectral tuning and the presence or absence of the phosphate-binding gene PstS.},
author = {Rusch, Douglas B and Halpern, Aaron L and Sutton, Granger and Heidelberg, Karla B and Williamson, Shannon and Yooseph, Shibu and Wu, Dongying and Eisen, Jonathan a and Hoffman, Jeff M and Remington, Karin and Beeson, Karen and Tran, Bao and Smith, Hamilton and Baden-Tillson, Holly and Stewart, Clare and Thorpe, Joyce and Freeman, Jason and Andrews-Pfannkoch, Cynthia and Venter, Joseph E and Li, Kelvin and Kravitz, Saul and Heidelberg, John F and Utterback, Terry and Rogers, Yu-Hui and Falc\'{o}n, Luisa I and Souza, Valeria and Bonilla-Rosso, Germ\'{a}n and Eguiarte, Luis E and Karl, David M and Sathyendranath, Shubha and Platt, Trevor and Bermingham, Eldredge and Gallardo, Victor and Tamayo-Castillo, Giselle and Ferrari, Michael R and Strausberg, Robert L and Nealson, Kenneth and Friedman, Robert and Frazier, Marvin and Venter, J Craig},
doi = {10.1371/journal.pbio.0050077},
file = {::},
issn = {1545-7885},
journal = {PLoS biology},
keywords = {Computational Biology,Food Chain,Oceans and Seas,Plankton,Species Specificity,Water Microbiology},
month = mar,
number = {3},
pages = {e77},
pmid = {17355176},
title = {{The Sorcerer II Global Ocean Sampling expedition: northwest Atlantic through eastern tropical Pacific.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1821060\&tool=pmcentrez\&rendertype=abstract},
volume = {5},
year = {2007}
}
@article{Sanderson2006,
author = {Sanderson, Stephanie and Boardman, Wayne and Ciofi, Claudio and Gibson, Richard},
doi = {10.1038/nature4441021a},
file = {::},
journal = {Nature},
title = {{Human gut microbes associated with obesity}},
volume = {444},
year = {2006}
}
@article{Sanger1992,
author = {Sanger, F and Nicklen, S and Coulson, a R},
file = {::},
issn = {0740-7378},
journal = {Biotechnology (Reading, Mass.)},
keywords = {Arabinonucleotides,Arabinonucleotides: metabolism,Bacteriophage phi X 174,Bacteriophage phi X 174: chemistry,Base Sequence,DNA, Viral,DNA, Viral: chemistry,Dideoxynucleosides,Dideoxynucleosides: metabolism,History, 20th Century,Molecular Sequence Data},
month = jan,
number = {12},
pages = {104--8},
pmid = {1422003},
title = {{DNA sequencing with chain-terminating inhibitors. 1977.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1422003},
volume = {24},
year = {1992}
}
@article{Shendure2008,
author = {Shendure, J and Ji, H},
file = {::},
issn = {1087-0156},
journal = {Nature Biotechnology},
month = oct,
number = {10},
pages = {1135--1145},
title = {{Next-generation DNA sequencing}},
url = {papers://1af565df-5311-4979-a8c5-a59321e8234a/Paper/p4821},
volume = {26},
year = {2008}
}
@article{Simpson2011,
annote = {T

        
Ideas:
   - filter out reads that have errors
   - construct an "FM-index" and do processing on that (not a DBG)

        
Takeaways:
   - trades computation time for memory -- more memory efficient but building FM-index is expensive.

        
Misc notes:
   - suggests ABySS and SOAPdenovo are better at handling large genomes and hence may be better for metagenomes},
author = {Simpson, J. T. and Durbin, R.},
doi = {10.1101/gr.126953.111},
file = {::},
issn = {1088-9051},
journal = {Genome Research},
month = dec,
title = {{Efficient de novo assembly of large genomes using compressed data structures}},
url = {http://genome.cshlp.org/cgi/doi/10.1101/gr.126953.111},
year = {2011}
}
@article{Simpson2009,
abstract = {Widespread adoption of massively parallel deoxyribonucleic acid (DNA) sequencing instruments has prompted the recent development of de novo short read assembly algorithms. A common shortcoming of the available tools is their inability to efficiently assemble vast amounts of data generated from large-scale sequencing projects, such as the sequencing of individual human genomes to catalog natural genetic variation. To address this limitation, we developed ABySS (Assembly By Short Sequences), a parallelized sequence assembler. As a demonstration of the capability of our software, we assembled 3.5 billion paired-end reads from the genome of an African male publicly released by Illumina, Inc. Approximately 2.76 million contigs > or =100 base pairs (bp) in length were created with an N50 size of 1499 bp, representing 68\% of the reference human genome. Analysis of these contigs identified polymorphic and novel sequences not present in the human reference assembly, which were validated by alignment to alternate human assemblies and to other primate genomes.},
author = {Simpson, Jared T and Wong, Kim and Jackman, Shaun D and Schein, Jacqueline E and Jones, Steven J M and Birol, Inan\c{c}},
doi = {10.1101/gr.089532.108},
file = {::},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Animals,Computational Biology,Computational Biology: methods,Contig Mapping,Escherichia coli K12,Escherichia coli K12: genetics,Genetic Variation,Genome, Human,Humans,Polymorphism, Genetic,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jun,
number = {6},
pages = {1117--23},
pmid = {19251739},
title = {{ABySS: a parallel assembler for short read sequence data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2694472\&tool=pmcentrez\&rendertype=abstract},
volume = {19},
year = {2009}
}
@article{Tata2006,
author = {Tata, S. and Friedman, J.S. and Swaroop, a.},
doi = {10.1109/ICDE.2006.47},
file = {::},
isbn = {0-7695-2570-9},
journal = {22nd International Conference on Data Engineering (ICDE'06)},
pages = {87--87},
publisher = {Ieee},
title = {{Declarative Querying for Biological Sequences}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1617455},
year = {2006}
}
@article{Tembe2010,
abstract = {SUMMARY: Large volumes of data generated by high-throughput sequencing instruments present non-trivial challenges in data storage, content access and transfer. We present G-SQZ, a Huffman coding-based sequencing-reads-specific representation scheme that compresses data without altering the relative order. G-SQZ has achieved from 65\% to 81\% compression on benchmark datasets, and it allows selective access without scanning and decoding from start. This article focuses on describing the underlying encoding scheme and its software implementation, and a more theoretical problem of optimal compression is out of scope. The immediate practical benefits include reduced infrastructure and informatics costs in managing and analyzing large sequencing data. AVAILABILITY: http://public.tgen.org/sqz. Academic/non-profit: Source: available at no cost under a non-open-source license by requesting from the web-site; Binary: available for direct download at no cost. For-Profit: Submit request for for-profit license from the web-site.},
author = {Tembe, Waibhav and Lowey, James and Suh, Edward},
doi = {10.1093/bioinformatics/btq346},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,DNA,DNA: methods,Data Compression,Sequence Analysis,Software},
month = sep,
number = {17},
pages = {2192--2194},
pmid = {20605925},
title = {{G-SQZ: compact encoding of genomic sequence and quality data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20605925},
volume = {26},
year = {2010}
}
@article{Tringe2005,
abstract = {Although genomics has classically focused on pure, easy-to-obtain samples, such as microbes that grow readily in culture or large animals and plants, these organisms represent only a fraction of the living or once-living organisms of interest. Many species are difficult to study in isolation because they fail to grow in laboratory culture, depend on other organisms for critical processes, or have become extinct. Methods that are based on DNA sequencing circumvent these obstacles, as DNA can be isolated directly from living or dead cells in various contexts. Such methods have led to the emergence of a new field, which is referred to as metagenomics.},
author = {Tringe, Susannah Green and Rubin, Edward M},
doi = {10.1038/nrg1709},
file = {::},
issn = {1471-0056},
journal = {Nature reviews. Genetics},
keywords = {Animals,Base Sequence,Base Sequence: genetics,Environmental Microbiology,Genetics, Microbial,Genetics, Microbial: methods,Genetics, Microbial: trends,Genomics,Genomics: methods,Genomics: trends,Humans,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Sequence Analysis, DNA: trends},
month = nov,
number = {11},
pages = {805--14},
pmid = {16304596},
title = {{Metagenomics: DNA sequencing of environmental samples.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16304596},
volume = {6},
year = {2005}
}
@article{Turnbaugh2009,
abstract = {The human distal gut harbours a vast ensemble of microbes (the microbiota) that provide important metabolic capabilities, including the ability to extract energy from otherwise indigestible dietary polysaccharides. Studies of a few unrelated, healthy adults have revealed substantial diversity in their gut communities, as measured by sequencing 16S rRNA genes, yet how this diversity relates to function and to the rest of the genes in the collective genomes of the microbiota (the gut microbiome) remains obscure. Studies of lean and obese mice suggest that the gut microbiota affects energy balance by influencing the efficiency of calorie harvest from the diet, and how this harvested energy is used and stored. Here we characterize the faecal microbial communities of adult female monozygotic and dizygotic twin pairs concordant for leanness or obesity, and their mothers, to address how host genotype, environmental exposure and host adiposity influence the gut microbiome. Analysis of 154 individuals yielded 9,920 near full-length and 1,937,461 partial bacterial 16S rRNA sequences, plus 2.14 gigabases from their microbiomes. The results reveal that the human gut microbiome is shared among family members, but that each person's gut microbial community varies in the specific bacterial lineages present, with a comparable degree of co-variation between adult monozygotic and dizygotic twin pairs. However, there was a wide array of shared microbial genes among sampled individuals, comprising an extensive, identifiable 'core microbiome' at the gene, rather than at the organismal lineage, level. Obesity is associated with phylum-level changes in the microbiota, reduced bacterial diversity and altered representation of bacterial genes and metabolic pathways. These results demonstrate that a diversity of organismal assemblages can nonetheless yield a core microbiome at a functional level, and that deviations from this core are associated with different physiological states (obese compared with lean).},
author = {Turnbaugh, Peter J and Hamady, Micah and Yatsunenko, Tanya and Cantarel, Brandi L and Duncan, Alexis and Ley, Ruth E and Sogin, Mitchell L and Jones, William J and Roe, Bruce a and Affourtit, Jason P and Egholm, Michael and Henrissat, Bernard and Heath, Andrew C and Knight, Rob and Gordon, Jeffrey I},
doi = {10.1038/nature07540},
file = {::},
issn = {1476-4687},
journal = {Nature},
keywords = {Adult,Africa,Africa: ethnology,Biodiversity,Environment,Europe,Europe: ethnology,Feces,Feces: microbiology,Female,Gastrointestinal Tract,Gastrointestinal Tract: microbiology,Genotype,Humans,Metagenome,Metagenome: genetics,Metagenome: physiology,Missouri,Molecular Sequence Data,Mothers,Obesity,Obesity: microbiology,RNA, Ribosomal, 16S,RNA, Ribosomal, 16S: analysis,RNA, Ribosomal, 16S: genetics,Thinness,Thinness: microbiology,Twins, Dizygotic,Twins, Monozygotic},
month = jan,
number = {7228},
pages = {480--4},
pmid = {19043404},
publisher = {Nature Publishing Group},
title = {{A core gut microbiome in obese and lean twins.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2677729\&tool=pmcentrez\&rendertype=abstract},
volume = {457},
year = {2009}
}
@article{VanLanen2006,
abstract = {The quest for the discovery of novel natural products has entered a new chapter with the enormous wealth of genetic data that is now available. This information has been exploited by using whole-genome sequence mining to uncover cryptic pathways, or biosynthetic pathways for previously undetected metabolites. Alternatively, using known paradigms for secondary metabolite biosynthesis, genetic information has been 'fished out' of DNA libraries resulting in the discovery of new natural products and isolation of gene clusters for known metabolites. Novel natural products have been discovered by expressing genetic data from uncultured organisms or difficult-to-manipulate strains in heterologous hosts. Furthermore, improvements in heterologous expression have not only helped to identify gene clusters but have also made it easier to manipulate these genes in order to generate new compounds. Finally, and perhaps the most crucial aspect of the efficient and prosperous use of the abundance of genetic information, novel enzyme chemistry continues to be discovered, which has aided our understanding of how natural products are biosynthesized de novo, and enabled us to rework the current paradigms for natural product biosynthesis.},
author = {{Van Lanen}, Steven G and Shen, Ben},
doi = {10.1016/j.mib.2006.04.002},
issn = {1369-5274},
journal = {Current opinion in microbiology},
keywords = {Bacteria,Bacteria: genetics,Bacteria: metabolism,Biological Factors,Biological Factors: chemistry,Biological Factors: genetics,Biological Factors: metabolism,Drug Industry,Drug Industry: methods,Genes, Bacterial,Genome, Bacterial,Genomics},
month = jun,
number = {3},
pages = {252--60},
pmid = {16651020},
title = {{Microbial genomics for the improvement of natural product discovery.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16651020},
volume = {9},
year = {2006}
}
@article{Venter2001,
abstract = {A 2.91-billion base pair (bp) consensus sequence of the euchromatic portion of the human genome was generated by the whole-genome shotgun sequencing method. The 14.8-billion bp DNA sequence was generated over 9 months from 27,271,853 high-quality sequence reads (5.11-fold coverage of the genome) from both ends of plasmid clones made from the DNA of five individuals. Two assembly strategies-a whole-genome assembly and a regional chromosome assembly-were used, each combining sequence data from Celera and the publicly funded genome effort. The public data were shredded into 550-bp segments to create a 2.9-fold coverage of those genome regions that had been sequenced, without including biases inherent in the cloning and assembly procedure used by the publicly funded group. This brought the effective coverage in the assemblies to eightfold, reducing the number and size of gaps in the final assembly over what would be obtained with 5.11-fold coverage. The two assembly strategies yielded very similar results that largely agree with independent mapping data. The assemblies effectively cover the euchromatic regions of the human chromosomes. More than 90\% of the genome is in scaffold assemblies of 100,000 bp or more, and 25\% of the genome is in scaffolds of 10 million bp or larger. Analysis of the genome sequence revealed 26,588 protein-encoding transcripts for which there was strong corroborating evidence and an additional approximately 12,000 computationally derived genes with mouse matches or other weak supporting evidence. Although gene-dense clusters are obvious, almost half the genes are dispersed in low G+C sequence separated by large tracts of apparently noncoding sequence. Only 1.1\% of the genome is spanned by exons, whereas 24\% is in introns, with 75\% of the genome being intergenic DNA. Duplications of segmental blocks, ranging in size up to chromosomal lengths, are abundant throughout the genome and reveal a complex evolutionary history. Comparative genomic analysis indicates vertebrate expansions of genes associated with neuronal function, with tissue-specific developmental regulation, and with the hemostasis and immune systems. DNA sequence comparisons between the consensus sequence and publicly funded genome data provided locations of 2.1 million single-nucleotide polymorphisms (SNPs). A random pair of human haploid genomes differed at a rate of 1 bp per 1250 on average, but there was marked heterogeneity in the level of polymorphism across the genome. Less than 1\% of all SNPs resulted in variation in proteins, but the task of determining which SNPs have functional consequences remains an open challenge.},
author = {Venter, J C and Adams, M D and Myers, E W and Li, P W and Mural, R J and Sutton, G G and Smith, H O and Yandell, M and Evans, C a and Holt, R a and Gocayne, J D and Amanatides, P and Ballew, R M and Huson, D H and Wortman, J R and Zhang, Q and Kodira, C D and Zheng, X H and Chen, L and Skupski, M and Subramanian, G and Thomas, P D and Zhang, J and {Gabor Miklos}, G L and Nelson, C and Broder, S and Clark, a G and Nadeau, J and McKusick, V a and Zinder, N and Levine, a J and Roberts, R J and Simon, M and Slayman, C and Hunkapiller, M and Bolanos, R and Delcher, a and Dew, I and Fasulo, D and Flanigan, M and Florea, L and Halpern, a and Hannenhalli, S and Kravitz, S and Levy, S and Mobarry, C and Reinert, K and Remington, K and Abu-Threideh, J and Beasley, E and Biddick, K and Bonazzi, V and Brandon, R and Cargill, M and Chandramouliswaran, I and Charlab, R and Chaturvedi, K and Deng, Z and {Di Francesco}, V and Dunn, P and Eilbeck, K and Evangelista, C and Gabrielian, a E and Gan, W and Ge, W and Gong, F and Gu, Z and Guan, P and Heiman, T J and Higgins, M E and Ji, R R and Ke, Z and Ketchum, K a and Lai, Z and Lei, Y and Li, Z and Li, J and Liang, Y and Lin, X and Lu, F and Merkulov, G V and Milshina, N and Moore, H M and Naik, a K and Narayan, V a and Neelam, B and Nusskern, D and Rusch, D B and Salzberg, S and Shao, W and Shue, B and Sun, J and Wang, Z and Wang, a and Wang, X and Wang, J and Wei, M and Wides, R and Xiao, C and Yan, C and Yao, a and Ye, J and Zhan, M and Zhang, W and Zhang, H and Zhao, Q and Zheng, L and Zhong, F and Zhong, W and Zhu, S and Zhao, S and Gilbert, D and Baumhueter, S and Spier, G and Carter, C and Cravchik, a and Woodage, T and Ali, F and An, H and Awe, a and Baldwin, D and Baden, H and Barnstead, M and Barrow, I and Beeson, K and Busam, D and Carver, a and Center, a and Cheng, M L and Curry, L and Danaher, S and Davenport, L and Desilets, R and Dietz, S and Dodson, K and Doup, L and Ferriera, S and Garg, N and Gluecksmann, a and Hart, B and Haynes, J and Haynes, C and Heiner, C and Hladun, S and Hostin, D and Houck, J and Howland, T and Ibegwam, C and Johnson, J and Kalush, F and Kline, L and Koduru, S and Love, a and Mann, F and May, D and McCawley, S and McIntosh, T and McMullen, I and Moy, M and Moy, L and Murphy, B and Nelson, K and Pfannkoch, C and Pratts, E and Puri, V and Qureshi, H and Reardon, M and Rodriguez, R and Rogers, Y H and Romblad, D and Ruhfel, B and Scott, R and Sitter, C and Smallwood, M and Stewart, E and Strong, R and Suh, E and Thomas, R and Tint, N N and Tse, S and Vech, C and Wang, G and Wetter, J and Williams, S and Williams, M and Windsor, S and Winn-Deen, E and Wolfe, K and Zaveri, J and Zaveri, K and Abril, J F and Guig\'{o}, R and Campbell, M J and Sjolander, K V and Karlak, B and Kejariwal, a and Mi, H and Lazareva, B and Hatton, T and Narechania, a and Diemer, K and Muruganujan, a and Guo, N and Sato, S and Bafna, V and Istrail, S and Lippert, R and Schwartz, R and Walenz, B and Yooseph, S and Allen, D and Basu, a and Baxendale, J and Blick, L and Caminha, M and Carnes-Stine, J and Caulk, P and Chiang, Y H and Coyne, M and Dahlke, C and Mays, a and Dombroski, M and Donnelly, M and Ely, D and Esparham, S and Fosler, C and Gire, H and Glanowski, S and Glasser, K and Glodek, a and Gorokhov, M and Graham, K and Gropman, B and Harris, M and Heil, J and Henderson, S and Hoover, J and Jennings, D and Jordan, C and Jordan, J and Kasha, J and Kagan, L and Kraft, C and Levitsky, a and Lewis, M and Liu, X and Lopez, J and Ma, D and Majoros, W and McDaniel, J and Murphy, S and Newman, M and Nguyen, T and Nguyen, N and Nodell, M and Pan, S and Peck, J and Peterson, M and Rowe, W and Sanders, R and Scott, J and Simpson, M and Smith, T and Sprague, a and Stockwell, T and Turner, R and Venter, E and Wang, M and Wen, M and Wu, D and Wu, M and Xia, a and Zandieh, a and Zhu, X},
doi = {10.1126/science.1058040},
file = {::},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
keywords = {Algorithms,Animals,Chromosome Banding,Chromosome Mapping,Chromosomes, Artificial, Bacterial,Computational Biology,Consensus Sequence,CpG Islands,DNA, Intergenic,Databases, Factual,Evolution, Molecular,Exons,Female,Gene Duplication,Genes,Genetic Variation,Genome, Human,Human Genome Project,Humans,Introns,Male,Phenotype,Physical Chromosome Mapping,Polymorphism, Single Nucleotide,Proteins,Proteins: genetics,Proteins: physiology,Pseudogenes,Repetitive Sequences, Nucleic Acid,Retroelements,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Species Specificity},
month = feb,
number = {5507},
pages = {1304--51},
pmid = {11181995},
title = {{The sequence of the human genome.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11181995},
volume = {291},
year = {2001}
}
@article{Voget2003,
author = {Voget, S and Leggewie, C and Uesbeck, A and Raasch, C and Jaeger, K and Streit, W R},
doi = {10.1128/AEM.69.10.6235},
file = {::},
journal = {Society},
number = {10},
title = {{Prospecting for Novel Biocatalysts in a Soil Metagenome Prospecting for Novel Biocatalysts in a Soil Metagenome}},
volume = {69},
year = {2003}
}
@article{VonMering2007,
abstract = {The taxonomic composition of environmental communities is an important indicator of their ecology and function. We used a set of protein-coding marker genes, extracted from large-scale environmental shotgun sequencing data, to provide a more direct, quantitative, and accurate picture of community composition than that provided by traditional ribosomal RNA-based approaches depending on the polymerase chain reaction. Mapping marker genes from four diverse environmental data sets onto a reference species phylogeny shows that certain communities evolve faster than others. The method also enables determination of preferred habitats for entire microbial clades and provides evidence that such habitat preferences are often remarkably stable over time.},
author = {von Mering, C and Hugenholtz, P and Raes, J and Tringe, S G and Doerks, T and Jensen, L J and Ward, N and Bork, P},
doi = {10.1126/science.1133420},
file = {::},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Animals,Bacteria,Bacteria: classification,Bacteria: genetics,Biological Evolution,Bone and Bones,Bone and Bones: microbiology,Ecosystem,Environmental Microbiology,Genes, Bacterial,Genes, rRNA,Genetic Markers,Genomics,Likelihood Functions,Mining,Phylogeny,Seawater,Seawater: microbiology,Soil Microbiology,Water Microbiology,Whales,Whales: microbiology},
month = feb,
number = {5815},
pages = {1126--30},
pmid = {17272687},
title = {{Quantitative phylogenetic assessment of microbial communities in diverse environments.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17272687},
volume = {315},
year = {2007}
}
@article{Wan2011,
abstract = {MOTIVATION: The growth of next generation sequencing means that more effective and efficient archiving methods are needed to store the generated data for public dissemination and in anticipation of more mature analytical methods later. This paper examines methods for compressing the quality score component of the data to partly address this problem. RESULTS: We compare several compression policies for quality scores, in terms of both compression effectiveness and overall efficiency. The policies employ lossy and lossless transformations with one of several coding schemes. Experiments show that both lossy and lossless transformations are useful, and that simple coding methods, which consume less computing resources, are highly competitive, especially when random access to reads is needed.Availability and Implementation: Our C++ implementation, released under the Lesser General Public License, is available for download at http://www.cb.k.u-tokyo.ac.jp/asailab/members/rwan. CONTACT: r-wan@cb.k.u-tokyo.ac.jp SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
annote = {Data types : FASTQ (qualities only)
Type of compression : lossless and lossy

        
Compression speed : slow
Decompression speed : fast

        
Other useful notes:

        
Method/approach:
   - convert the quality scores back into estimates of the probability and then quantize the probability [0,1] differently
   - this reduces the possible values for the quality scores
   - apply "standard" compression tools like gzip

        
Comments:
   - next to last graf of intro has applications for qualities (Blankenberg et al. 2010) (Li et al. 2008)
   - Survey on compression in comp bio Giancarlo et al 2009},
author = {Wan, Raymond and Anh, Vo Ngoc and Asai, Kiyoshi},
doi = {10.1093/bioinformatics/btr689},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = dec,
number = {5},
pages = {628--635},
pmid = {22171329},
title = {{Transformations for the Compression of FASTQ Quality Scores of Next Generation Sequencing Data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22171329},
volume = {28},
year = {2011}
}
@inproceedings{Wan2010,
annote = {Data types : FASTQ
Reference-based : no
Type of compression : lossless

        

        
Method/approach:
   - resort the reads and then apply GZIP or BZIP
   - shows file size can be reduced by 5-10\% more from ordering (!)

      },
author = {Wan, Raymond and Asai, Kiyoshi},
booktitle = {Proceedings of the 2010 IEEE International Conference on Bioinformatics and Biomedicine Workshops},
file = {::},
isbn = {9781424483020},
keywords = {-next generation sequencing,data,data archiving,pre-processing,sorting},
number = {2},
pages = {567--572},
title = {{Sorting Next Generation Sequencing Data Improves Compression Effectiveness}},
year = {2010}
}
@article{Wang2011,
abstract = {With the advent of DNA sequencing technologies, more and more reference genome sequences are available for many organisms. Analyzing sequence variation and understanding its biological importance are becoming a major research aim. However, how to store and process the huge amount of eukaryotic genome data, such as those of the human, mouse and rice, has become a challenge to biologists. Currently available bioinformatics tools used to compress genome sequence data have some limitations, such as the requirement of the reference single nucleotide polymorphisms (SNPs) map and information on deletions and insertions. Here, we present a novel compression tool for storing and analyzing Genome ReSequencing data, named GRS. GRS is able to process the genome sequence data without the use of the reference SNPs and other sequence variation information and automatically rebuild the individual genome sequence data using the reference genome sequence. When its performance was tested on the first Korean personal genome sequence data set, GRS was able to achieve ∼159-fold compression, reducing the size of the data from 2986.8 to 18.8 MB. While being tested against the sequencing data from rice and Arabidopsis thaliana, GRS compressed the 361.0 MB rice genome data to 4.4 MB, and the A. thaliana genome data from 115.1 MB to 6.5 KB. This de novo compression tool is available at http://gmdd.shgmo.org/Computational-Biology/GRS.},
author = {Wang, Congmao and Zhang, Dabing},
doi = {10.1093/nar/gkr009},
file = {::},
issn = {1362-4962},
journal = {Nucleic Acids Research},
keywords = {Arabidopsis,Arabidopsis: genetics,DNA,Genome,Genomics,Genomics: methods,Human,Humans,Oryza sativa,Oryza sativa: genetics,Plant,Sequence Analysis,Software},
month = apr,
number = {7},
pages = {e45},
pmid = {21266471},
title = {{A novel compression tool for efficient storage of genome resequencing data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3074166\&tool=pmcentrez\&rendertype=abstract},
volume = {39},
year = {2011}
}
@article{White2008,
abstract = {Publicly available DNA sequence databases such as GenBank are large, and are growing at an exponential rate. The sheer volume of data being dealt with presents serious storage and data communications problems. Currently, sequence data is usually kept in large "flat files," which are then compressed using standard Lempel-Ziv (gzip) compression - an approach which rarely achieves good compression ratios. While much research has been done on compressing individual DNA sequences, surprisingly little has focused on the compression of entire databases of such sequences. In this study we introduce the sequence database compression software coil.},
annote = {Data types : FASTA
Reference-based : no
Type of compression : lossless

        
Compression speed : slow
Decompression speed : fast

        
Method/approach:
   - creates a "similarity graph" from the sequences
   - builds a forest, effectively clustering the graph
   - compress based on changes made to traverse the tree

        
Comments:
   - clever approach, but will work well only if data is clusterable
   - Pointers to other literature on compression : [5]-[9],[12]},
author = {White, W Timothy J and Hendy, Michael D},
doi = {10.1186/1471-2105-9-242},
file = {::},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Animals,DNA,Data Compression,Data Compression: methods,Database Management Systems,Databases,Evolution,Expressed Sequence Tags,Humans,Molecular,Neural Networks (Computer),Nucleic Acid,Phylogeny,Point Mutation,Sequence Analysis,Species Specificity},
month = jan,
pages = {242},
pmid = {18489794},
title = {{Compressing DNA sequence databases with coil}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2426707\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Zerbino2010,
abstract = {The Velvet de novo assembler was designed to build contigs and eventually scaffolds from short-read sequencing data. This protocol describes how to use Velvet, interpret its output, and tune its parameters for optimal results. It also covers practical issues such as configuration, using the VelvetOptimiser routine, and processing colorspace data.},
author = {Zerbino, Daniel R},
doi = {10.1002/0471250953.bi1105s31},
file = {::},
isbn = {0471250953},
issn = {1934-340X},
journal = {Current protocols in bioinformatics},
keywords = {Base Sequence,DNA,DNA: methods,Sequence Analysis,Software},
month = sep,
number = {September},
pages = {Unit 11.5},
pmid = {20836074},
title = {{Using the Velvet de novo assembler for short-read sequencing technologies.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2952100\&tool=pmcentrez\&rendertype=abstract},
volume = {Chapter 11},
year = {2010}
}
@article{Zerbino2008,
abstract = {We have developed a new set of algorithms, collectively called "Velvet," to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (k-mers) that is ideal for high coverage, very short read (25-50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of approximately 8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies.},
author = {Zerbino, Daniel R and Birney, Ewan},
doi = {10.1101/gr.074492.107},
file = {::},
issn = {1088-9051},
journal = {Genome Research},
keywords = {Algorithms,Animals,Artificial,Bacterial,Chromosomes,Computational Biology,Computational Biology: methods,Computer Simulation,DNA,DNA: methods,DNA: standards,Genome,Genomics,Human,Humans,Mammals,Mammals: genetics,Sequence Analysis,Streptococcus,Streptococcus: genetics},
month = may,
number = {5},
pages = {821--829},
pmid = {18349386},
title = {{Velvet: algorithms for de novo short read assembly using de Bruijn graphs.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2336801\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Zerbino2009,
abstract = {Despite the short length of their reads, micro-read sequencing technologies have shown their usefulness for de novo sequencing. However, especially in eukaryotic genomes, complex repeat patterns are an obstacle to large assemblies.},
author = {Zerbino, Daniel R and McEwen, Gayle K and Margulies, Elliott H and Birney, Ewan},
doi = {10.1371/journal.pone.0008407},
file = {::},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Animals,Chromosomes, Artificial, Bacterial,Chromosomes, Artificial, Bacterial: genetics,Computer Simulation,Ferrets,Ferrets: genetics,Pseudomonas syringae,Pseudomonas syringae: genetics,Repetitive Sequences, Nucleic Acid,Repetitive Sequences, Nucleic Acid: genetics,Sequence Analysis, DNA,Sequence Analysis, DNA: instrumentation,Sequence Analysis, DNA: methods},
month = jan,
number = {12},
pages = {e8407},
pmid = {20027311},
title = {{Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2793427\&tool=pmcentrez\&rendertype=abstract},
volume = {4},
year = {2009}
}
@phdthesis{Zerbino2009,
author = {Zerbino, Daniel Robert},
booktitle = {Molecular Biology},
file = {::},
school = {University of Cambridge},
title = {{Genome assembly and comparison using de Bruijn graphs}},
type = {PhD},
year = {2009}
}
@article{Zhang2009,
author = {Zhang, Jin and Wu, Jiangxing and Lan, Julong and Liu, Jianqiang},
doi = {10.1007/s11767-008-0031-x},
file = {:Users/dcjones/pap/Zhang et al. - 2009 - Performance evaluation and comparison of three counting bloom filter schemes.pdf:pdf},
issn = {0217-9822},
journal = {Journal of Electronics (China)},
month = may,
number = {3},
pages = {332--340},
title = {{Performance evaluation and comparison of three counting Bloom filter schemes}},
url = {http://www.springerlink.com/index/10.1007/s11767-008-0031-x},
volume = {26},
year = {2009}
}
@article{Bonomi2006,
author = {Bonomi, Flavio and Mitzenmacher, Michael and Panigrahy, Rina},
journal = {14th Annual European Symposium on Algorithms, LNCS 4168},
pages = {684--695},
publisher = {Springer},
title = {{An improved construction for counting Bloom filters}},
url = {http://www.springerlink.com/index/918u04172g176878.pdf},
year = {2006}
}
@article{Liu2011,
abstract = {Next-generation sequencing technologies have given rise to the explosive increase in DNA sequencing throughput, and have promoted the recent development of de novo short read assemblers. However, existing assemblers require high execution times and a large amount of compute resources to assemble large genomes from quantities of short reads.},
author = {Liu, Yongchao and Schmidt, Bertil and Maskell, Douglas L},
doi = {10.1186/1471-2105-12-354},
file = {:Users/dcjones/pap/Liu, Schmidt, Maskell - 2011 - Parallelized short read assembly of large genomes using de Bruijn graphs.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Bacteria,Bacteria: genetics,Computational Biology,Computational Biology: methods,Genome,Genome, Human,High-Throughput Nucleotide Sequencing,Humans,Software},
month = jan,
number = {1},
pages = {354},
pmid = {21867511},
publisher = {BioMed Central Ltd},
title = {{Parallelized short read assembly of large genomes using de Bruijn graphs.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3167803\&tool=pmcentrez\&rendertype=abstract},
volume = {12},
year = {2011}
}
@article{Cock2010,
abstract = {FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.},
author = {Cock, Peter J. A. and Fields, Christopher J. and Goto, Naohisa and Heuer, Michael L. and Rice, Peter M.},
doi = {10.1093/nar/gkp1137},
file = {:Users/dcjones/pap/Cock et al. - 2010 - The Sanger FASTQ file format for sequences with quality scores, and the SolexaIllumina FASTQ variants.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic Acids Research},
keywords = {Computational Biology,Computational Biology: history,History, 20th Century,History, 21st Century,Sequence Analysis, DNA,Sequence Analysis, DNA: history,Sequence Analysis, DNA: standards,Software},
month = apr,
number = {6},
pages = {1767--71},
pmid = {20015970},
title = {{The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2847217\&tool=pmcentrez\&rendertype=abstract},
volume = {38},
year = {2010}
}
@misc{Goby2012,
author = {{Campagne Laboratory, Institute for Computational Biology, Weill Cornell Medical School}},
title = {Goby},
month = may,
year  = {2012},
url   = {http://campagnelab.org/software/goby/}
}
@misc{Hach2012,
author = {Hach, Faraz and Numanagic, Ibrahim and Alkan, Can},
title  = {{SCALCE: Boosting Sequence Compression Algorithms using Locally Consistent Encoding}},
month  = map,
year   = {2012},
url    = {http://scalce.sourceforge.net}
}
@article{Christley2009,
abstract = {SUMMARY: The amount of genomic sequence data being generated and made available through public databases continues to increase at an ever-expanding rate. Downloading, copying, sharing and manipulating these large datasets are becoming difficult and time consuming for researchers. We need to consider using advanced compression techniques as part of a standard data format for genomic data. The inherent structure of genome data allows for more efficient lossless compression than can be obtained through the use of generic compression programs. We apply a series of techniques to James Watson's genome that in combination reduce it to a mere 4MB, small enough to be sent as an email attachment.},
author = {Christley, Scott and Lu, Yiming and Li, Chen and Xie, Xiaohui},
doi = {10.1093/bioinformatics/btn582},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Data Compression,Data Compression: methods,Databases, Genetic,Electronic Mail,Genome, Human,Humans},
month = jan,
number = {2},
pages = {274--5},
pmid = {18996942},
title = {{Human genomes as email attachments.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18996942},
volume = {25},
year = {2009}
}
@article{Melsted2011,
author = {Melsted, Pall and Pritchard, Jonathan K},
doi = {10.1186/1471-2105-12-333},
file = {:Users/dcjones/pap/Melsted, Pritchard - 2011 - Efficient counting of k-mers in DNA sequences using a Bloom Filter.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {333},
publisher = {BioMed Central Ltd},
title = {{Efficient counting of k-mers in DNA sequences using a Bloom Filter}},
url = {http://www.biomedcentral.com/1471-2105/12/333},
volume = {12},
year = {2011}
}
@article{Pell2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1112.4193v2},
author = {Pell, Jason and Hintze, Arend and Canino-Koning, R and Howe, Adina},
eprint = {arXiv:1112.4193v2},
file = {:Users/dcjones/pap/Pell et al. - 2011 - Scaling metagenome sequence assembly with probabilistic de Bruijn graphs.pdf:pdf},
journal = {Arxiv preprint arXiv:},
number = {1},
pages = {1--11},
title = {{Scaling metagenome sequence assembly with probabilistic de Bruijn graphs}},
url = {http://arxiv.org/abs/1112.4193},
volume = {I},
year = {2011}
}
@article{Sakib2011,
author = {Sakib, Muhammad Nazmus and Tang, Jijun and Zheng, W. Jim and Huang, Chin-Tser},
doi = {10.1371/journal.pone.0028251},
editor = {Mari\~{n}o-Ram\'{\i}rez, Leonardo},
file = {:Users/dcjones/pap/Sakib et al. - 2011 - Improving Transmission Efficiency of Large Sequence AlignmentMap (SAM) Files.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
month = dec,
number = {12},
pages = {e28251},
title = {{Improving Transmission Efficiency of Large Sequence Alignment/Map (SAM) Files}},
url = {http://dx.plos.org/10.1371/journal.pone.0028251},
volume = {6},
year = {2011}
}
@article{Fan2000,
author = {Fan, Li and Cao, Pei and Almeida, J. and Broder, A.Z.},
doi = {10.1109/90.851975},
file = {:Users/dcjones/pap/Fan et al. - 2000 - Summary cache a scalable wide-area Web cache sharing protocol.pdf:pdf},
issn = {10636692},
journal = {IEEE/ACM Transactions on Networking},
month = jun,
number = {3},
pages = {281--293},
title = {{Summary cache: a scalable wide-area Web cache sharing protocol}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=851975},
volume = {8},
year = {2000}
}
@article{Bloom1970,
author = {Bloom, Burton H.},
doi = {10.1145/362686.362692},
file = {:Users/dcjones/pap/Bloom - 1970 - Spacetime trade-offs in hash coding with allowable errors.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {and phrases,hash addressing,hash coding,retrieval efficiency,retrieval trade-offs,scatter storage,searching,storage,storage layout},
month = jul,
number = {7},
pages = {422--426},
title = {{Space/time trade-offs in hash coding with allowable errors}},
url = {http://portal.acm.org/citation.cfm?doid=362686.362692},
volume = {13},
year = {1970}
}
@article{Cox2012,
author = {Cox, A. J. and Bauer, M. J. and Jakobi, T. and Rosone, G.},
doi = {10.1093/bioinformatics/bts173},
file = {:Users/dcjones/pap/Cox et al. - 2012 - Large-scale compression of genomic sequence databases with the Burrows-Wheeler transform.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = may,
number = {11},
pages = {1415--1419},
title = {{Large-scale compression of genomic sequence databases with the Burrows-Wheeler transform}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/bts173},
volume = {28},
year = {2012}
}
@article{Said2004,
author = {Said, Amir},
file = {:Users/dcjones/pap/Said - 2004 - Introduction to Arithmetic Coding - Theory and Practice.pdf:pdf},
journal = {Hewlett-Packard Laboratories Report},
title = {{Introduction to Arithmetic Coding - Theory and Practice}},
volume = {HPL-2004-7},
year = {2004}
}
@article{Wu2010,
abstract = {Next-generation sequencing captures sequence differences in reads relative to a reference genome or transcriptome, including splicing events and complex variants involving multiple mismatches and long indels. We present computational methods for fast detection of complex variants and splicing in short reads, based on a successively constrained search process of merging and filtering position lists from a genomic index. Our methods are implemented in GSNAP (Genomic Short-read Nucleotide Alignment Program), which can align both single- and paired-end reads as short as 14 nt and of arbitrarily long length. It can detect short- and long-distance splicing, including interchromosomal splicing, in individual reads, using probabilistic models or a database of known splice sites. Our program also permits SNP-tolerant alignment to a reference space of all possible combinations of major and minor alleles, and can align reads from bisulfite-treated DNA for the study of methylation state.},
author = {Wu, Thomas D. and Nacu, Serban},
doi = {10.1093/bioinformatics/btq057},
file = {:Users/dcjones/pap/Wu, Nacu - 2010 - Fast and SNP-tolerant detection of complex variants and splicing in short reads.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Base Sequence,DNA, Recombinant,Genetic Variation,Genomics,Genomics: methods,Polymorphism, Single Nucleotide,RNA Splicing},
month = apr,
number = {7},
pages = {873--81},
pmid = {20147302},
title = {{Fast and SNP-tolerant detection of complex variants and splicing in short reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2844994\&tool=pmcentrez\&rendertype=abstract},
volume = {26},
year = {2010}
}
@article{Asmann2012,
abstract = {Fusion genes and fusion gene products are widely employed as biomarkers and therapeutic targets in hematopoietic cancers, but their applications have yet to be appreciated in solid tumors. Here, we report the use of SnowShoes-FTD, a powerful new analytic pipeline that can identify fusion transcripts and assess their redundancy and tumor subtype-specific distribution in primary tumors. In a study of primary breast tumors, SnowShoes-FTD was used to analyze paired-end mRNA-Seq data from a panel of estrogen receptor (ER)(+), HER2(+), and triple-negative primary breast tumors, identifying tumor-specific fusion transcripts by comparison with mRNA-Seq data from nontransformed human mammary epithelial cell cultures plus the Illumina Body Map data from normal tissues. We found that every primary breast tumor that was analyzed expressed one or more fusion transcripts. Of the 131 tumor-specific fusion transcripts identified, 86 were "private" (restricted to a single tumor) and 45 were "redundant" (distributed among multiple tumors). Among the redundant fusion transcripts, 7 were unique to ER(+) tumors and 8 were unique to triple-negative tumors. In contrast, none of the redundant fusion transcripts were unique to HER2(+) tumors. Both private and redundant fusion transcripts were widely expressed in primary breast tumors, with many mapping to genomic loci implicated in breast carcinogenesis and/or risk. Our finding that some fusion transcripts are tumor subtype-specific suggests that these entities may be critical determinants in the etiology of breast cancer subtypes, useful as biomarkers for tumor stratification, or exploitable as cancer-specific therapeutic targets. Cancer Res; 72(8); 1921-8. ©2012 AACR.},
author = {Asmann, Yan W and Necela, Brian M and Kalari, Krishna R and Hossain, Asif and Baker, Tiffany R and Carr, Jennifer M and Davis, Caroline and Getz, Julie E and Hostetter, Galen and Li, Xing and McLaughlin, Sarah a and Radisky, Derek C and Schroth, Gary P and Cunliffe, Heather E and Perez, Edith a and Thompson, E Aubrey},
doi = {10.1158/0008-5472.CAN-11-3142},
file = {:Users/dcjones/pap/Asmann, Necela, Kalari - 2012 - Detection of Redundant Fusion Transcripts as Biomarkers or Disease-Specific Therapeutic Targets in Breast Cancer Detection of Redundant Fusion Transcripts as Biomarkers or Disease-Specific Therapeutic Targets in Breast.pdf:pdf},
issn = {1538-7445},
journal = {Cancer research},
month = apr,
pages = {1921--1928},
pmid = {22496456},
title = {{Detection of Redundant Fusion Transcripts as Biomarkers or Disease-Specific Therapeutic Targets in Breast Cancer.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22496456},
year = {2012}
}
@article{Denef2010,
abstract = {Similar to virtually all components of natural environments, microbial systems are inherently complex and dynamic. Advances in cultivation-independent molecular methods have provided a route to study microbial consortia in their natural surroundings and to begin resolving the community structure, dominant metabolic processes and inter-organism interactions. However, the utility of these methods generally scales inversely with community complexity. By applying genomics-enabled methods to the study of natural microbial communities with reduced levels of species richness, a relatively comprehensive understanding of the metabolic networks and evolutionary processes within these communities can be attained. In such well-defined model systems, it is also possible to link emergent ecological patterns to their molecular and evolutionary underpinnings, facilitating construction of predictive ecosystem models. In this study, we review over a decade of research on one such system-acid mine drainage biofilm communities. We discuss the value and limitations of tractable model microbial communities in developing molecular methods for microbial ecology and in uncovering principles that may explain behavior in more complex systems.},
author = {Denef, Vincent J and Mueller, Ryan S and Banfield, Jillian F},
doi = {10.1038/ismej.2009.158},
file = {:Users/dcjones/pap/Denef, Mueller, Banfield - 2010 - AMD biofilms using model communities to study microbial evolution and ecological complexity in nature.pdf:pdf},
issn = {1751-7370},
journal = {The ISME Journal},
keywords = {Archaea,Archaea: genetics,Archaea: physiology,Archaea: virology,Bacteria,Bacteria: genetics,Bacteria: virology,Bacterial Physiological Phenomena,Biofilms,Biological,Ecosystem,Metagenome,Mining,Models},
month = may,
number = {5},
pages = {599--610},
pmid = {20164865},
publisher = {Nature Publishing Group},
title = {{AMD biofilms: using model communities to study microbial evolution and ecological complexity in nature.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20164865},
volume = {4},
year = {2010}
}
@article{The1000GenomesConsortium2010,
abstract = {The 1000 Genomes Project aims to provide a deep characterization of human genome sequence variation as a foundation for investigating the relationship between genotype and phenotype. Here we present results of the pilot phase of the project, designed to develop and compare different strategies for genome-wide sequencing with high-throughput platforms. We undertook three projects: low-coverage whole-genome sequencing of 179 individuals from four populations; high-coverage sequencing of two mother-father-child trios; and exon-targeted sequencing of 697 individuals from seven populations. We describe the location, allele frequency and local haplotype structure of approximately 15 million single nucleotide polymorphisms, 1 million short insertions and deletions, and 20,000 structural variants, most of which were previously undescribed. We show that, because we have catalogued the vast majority of common variation, over 95\% of the currently accessible variants found in any individual are present in this data set. On average, each person is found to carry approximately 250 to 300 loss-of-function variants in annotated genes and 50 to 100 variants previously implicated in inherited disorders. We demonstrate how these results can be used to inform association and functional studies. From the two trios, we directly estimate the rate of de novo germline base substitution mutations to be approximately 10(-8) per base pair per generation. We explore the data with regard to signatures of natural selection, and identify a marked reduction of genetic variation in the neighbourhood of genes, due to selection at linked sites. These methods and public data will support the next phase of human genetic research.},
author = {{The 1000 Genomes Consortium}},
doi = {10.1038/nature09534},
file = {:Users/dcjones/pap/The 1000 Genomes Consortium - 2010 - A map of human genome variation from population-scale sequencing.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Calibration,Chromosomes,Computational Biology,DNA,DNA Mutational Analysis,DNA: methods,Evolution,Female,Genetic,Genetic Association Studies,Genetic Variation,Genetic Variation: genetics,Genetic: genetics,Genetics,Genome,Genome-Wide Association Study,Genomics,Genomics: methods,Genotype,Haplotypes,Haplotypes: genetics,Human,Human: genetics,Humans,Male,Mitochondrial,Mitochondrial: genetics,Molecular,Mutation,Mutation: genetics,Pilot Projects,Polymorphism,Population,Population: methods,Recombination,Sample Size,Selection,Sequence Alignment,Sequence Analysis,Single Nucleotide,Single Nucleotide: genetics,Y,Y: genetics},
month = oct,
number = {7319},
pages = {1061--73},
pmid = {20981092},
title = {{A map of human genome variation from population-scale sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3042601\&tool=pmcentrez\&rendertype=abstract},
volume = {467},
year = {2010}
}
@article{Little2011,
abstract = {Runx2 is a metastatic transcription factor (TF) increasingly expressed during prostate cancer (PCa) progression. Using PCa cells conditionally expressing Runx2, we previously identified Runx2-regulated genes with known roles in epithelial-mesenchymal transition, invasiveness, angiogenesis, extracellular matrix proteolysis and osteolysis. To map Runx2-occupied regions (R2ORs) in PCa cells, we first analyzed regions predicted to bind Runx2 based on the expression data, and found that recruitment to sites upstream of the KLK2 and CSF2 genes was cyclical over time. Genome-wide ChIP-seq analysis at a time of maximum occupancy at these sites revealed 1603 high-confidence R2ORs, enriched with cognate motifs for RUNX, GATA and ETS TFs. The R2ORs were distributed with little regard to annotated transcription start sites (TSSs), mainly in introns and intergenic regions. Runx2-upregulated genes, however, displayed enrichment for R2ORs within 40 kb of their TSSs. The main annotated functions enriched in 98 Runx2-upregulated genes with nearby R2ORs were related to invasiveness and membrane trafficking/secretion. Indeed, using SDS-PAGE, mass spectrometry and western analyses, we show that Runx2 enhances secretion of several proteins, including fatty acid synthase and metastasis-associated laminins. Thus, combined analysis of Runx2's transcriptome and genomic occupancy in PCa cells lead to defining its novel role in regulating protein secretion.},
author = {Little, Gillian H and Noushmehr, Houtan and Baniwal, Sanjeev K and Berman, Benjamin P and Coetzee, Gerhard a and Frenkel, Baruch},
doi = {10.1093/nar/gkr1219},
file = {:Users/dcjones/pap/Little et al. - 2011 - Genome-wide Runx2 occupancy in prostate cancer cells suggests a role in regulating secretion.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
month = dec,
pages = {1--10},
pmid = {22187159},
title = {{Genome-wide Runx2 occupancy in prostate cancer cells suggests a role in regulating secretion.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22187159},
year = {2011}
}
